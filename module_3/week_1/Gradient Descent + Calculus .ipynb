{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Objectives \n",
    "* Understand the general process of gradient descent with respect to RSS(cost function) \n",
    "* Understand how derivatives are used in gradient descent \n",
    "* Use OOP to apply gradient descent in linear regression \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to find the optimal slope and intercept of a line of best fit using RSS as our cost function \n",
    "\n",
    "1. Take the derivative of the loss function for each parameter(gradient).\n",
    "2. Pick random values for the parameters. \n",
    "3. Plug the parameter values into the derivatives. \n",
    "4. Calculate the step sizes (slope * learning rate) \n",
    "5. Calculate new parameters (old parameters - step size) \n",
    "6. Repeat steps 3-5 until max number of steps is reached or minimum step size is reached. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives in gradient descent \n",
    "**A derivative tells us how a function is changing at any given point in time. They calculate the rate of change** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules for taking Derivatives \n",
    "\n",
    "1. **Power Rule** - $$f(x) = x^r $$\n",
    "\n",
    "Then, the derivative is: \n",
    "$$ f'(x) = r*x^{r-1} $$\n",
    "\n",
    "2. **Constant factor rule** - $$f(x) = 2x^2 $$\n",
    "\n",
    "\n",
    "$$f'(x) = 2*\\frac{\\Delta f}{\\Delta x} x^{2} = 2*2*x^{2-1} = 4x^1 = 4x $$\n",
    "\n",
    "3. **Addition Rule** - To take a derivative of a function that has multiple terms, simply take the derivative of each of the terms individually.  So for the function above, \n",
    "\n",
    "$$ f(x) = 4x^3 - x^2 + 3x $$\n",
    "\n",
    "$$ f'(x) = 12x^2 - 2x + 3  $$  \n",
    "\n",
    "4. **Chain Rule** - allows us to take partial derivatives of a function with respect to the other variables. See [learn.co lesson](https://learn.co/tracks/module-3-data-science-career-2-1/appendix/more-on-derivatives/derivatives-the-chain-rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOP gradient descent with Linear Regression using MSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
