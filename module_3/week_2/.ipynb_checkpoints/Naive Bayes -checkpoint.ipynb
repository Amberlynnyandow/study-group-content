{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes \n",
    "**A parametric, eager learning classifier based on Bayes' theorem with an assumption of independence among the predictors** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remeber Bayes Theorem? \n",
    "P(H|E) = (P(E|H) * P(H)) / P(E)\n",
    "\n",
    "**where:**\n",
    "\n",
    "* P(H|E) is the probability of hypothesis H given the event E, a posterior probability.\n",
    "* P(E|H) is the probability of event E given that the hypothesis H is true.\n",
    "* P(H) is the probability of hypothesis H being true (regardless of any related event), or prior probability of H.\n",
    "* P(E) is the probability of the event occurring (regardless of the hypothesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at an example: \n",
    "Let's say that we are interested in knowing whether an e-mail that contains the word sex (event) is spam (hypothesis). If we go back to the theorem description, this problem can be formulated as:\n",
    "\n",
    "**P(class=SPAM|contains=\"sex\") = (P(contains=\"sex\"|class=SPAM) * P(class=SPAM)) / P(contains=\"sex\")**\n",
    "\n",
    "The probability of an e-mail containing the word sex being spam is equal to the proportion of SPAM emails that contain the word sex multiplied by the proportion of e-mails being spam and divided by the proportion of e-mails containing the word sex.\n",
    "\n",
    "Let's dissect this:\n",
    "\n",
    "* P(class=SPAM|contains=\"sex\") is the probability of an e-mail being SPAM given that this e-mail contains the word sex. This is what we are interested in predicting.\n",
    "* P(contains=\"sex\"|class=SPAM) is the probability of an e-mail containing the word sex given that this e-mail has been recognized as SPAM. This is our training data, which represents the correlation between an e-mail being considered SPAM and such e-mail containing the word sex.\n",
    "* P(class=SPAM) is the probability of an e-mail being SPAM (without any prior knowledge of the words it contains). This is simply the proportion of e-mails being SPAM in our entire training set. We multiply by this value because we are interested in knowing how significant is information concerning SPAM e-mails. If this value is low, the significance of any events related to SPAM e-mails will also be low.\n",
    "* P(contains=\"sex\") is the probability of an e-mail containing the word sex. This is simply the proportion of e-mails containing the word sex in our entire training set. We divide by this value because the more exclusive the word sex is, the more important is the context in which it appears. Thus, if this number is low (the word appears very rarely), it can be a great indicator that in the cases it does appear, it is a relevant feature to analyze.\n",
    "\n",
    "In summary, the Bayes Theorem allows us to make reasoned deduction of events happening in the real world based on prior knowledge of observations that may imply it. To apply this theorem to any problem, we need to compute the two types of probabilities that appear in the formula.\n",
    "\n",
    "**Class Probabilities**\n",
    "\n",
    "In the theorem, P(A) represents the probabilities of each event. In the Naive Bayes Classifier, we can interpret these Class Probabilities as simply the frequency of each instance of the event divided by the total number of instances. For example, in the previous example of spam detection, P(class=SPAM) represents the number of e-mails classified as spam divided by the sum of all instances (this is spam + not spam)\n",
    "\n",
    "**P(class=SPAM) = count(class=SPAM) / (count(class=notSPAM) + count(class=SPAM))**\n",
    "\n",
    "**Conditional Probabilities**\n",
    "\n",
    "In the theorem, P(A|B) represents the conditional probabilities of an event A given another event B. In the Naive Bayes Classifier, these encode the posterior probability of A occurring when B is true.\n",
    "\n",
    "For the spam example, P(class=SPAM|contains=\"sex\") represents the number of instances in which an e-mail is considered as spam and contains the word sex, divided by the total number of e-mails that contain the word sex:\n",
    "\n",
    "**P(class=SPAM|contains=\"sex\") = count(class=SPAM & contains=sex) / count(contains=sex)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Bayes \n",
    "\n",
    "1. Calculate prior probability for given class labels \n",
    "2. Calculate conditional probability with each attribute for each class \n",
    "3. Multiply same class conditional probability \n",
    "4. Multiply prior probability with step 3 probability \n",
    "5. See which class has a higher probability, higher probability class belongs to given input set step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T15:49:16.758282Z",
     "start_time": "2020-05-15T15:49:15.762725Z"
    }
   },
   "outputs": [],
   "source": [
    "#load libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#load the dataset\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "#Print the first 5 rows of the dataframe.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T15:49:16.767215Z",
     "start_time": "2020-05-15T15:49:16.760008Z"
    }
   },
   "outputs": [],
   "source": [
    "#replace 0 with NaNs because they will be easier to replace and detect\n",
    "df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n",
    "## showing the count of Nans\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T15:49:20.855334Z",
     "start_time": "2020-05-15T15:49:20.848804Z"
    }
   },
   "outputs": [],
   "source": [
    "#replacing 0's with appropriate stats\n",
    "df['Glucose'].fillna(df['Glucose'].mean(), inplace = True)\n",
    "df['BloodPressure'].fillna(df['BloodPressure'].mean(), inplace = True)\n",
    "df['SkinThickness'].fillna(df['SkinThickness'].median(), inplace = True)\n",
    "df['Insulin'].fillna(df['Insulin'].median(), inplace = True)\n",
    "df['BMI'].fillna(df['BMI'].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T15:49:23.070794Z",
     "start_time": "2020-05-15T15:49:23.066459Z"
    }
   },
   "outputs": [],
   "source": [
    "#create target and features \n",
    "X = df.drop('Outcome',axis=1).values\n",
    "y = df['Outcome'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T15:57:09.123789Z",
     "start_time": "2020-05-15T15:57:08.804368Z"
    }
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "ax1 = sns.heatmap(corr, annot=True, cmap='Blues')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T15:57:18.136893Z",
     "start_time": "2020-05-15T15:57:17.632879Z"
    }
   },
   "outputs": [],
   "source": [
    "#importing train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,\n",
    "                                                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T15:58:12.463578Z",
     "start_time": "2020-05-15T15:58:12.458967Z"
    }
   },
   "outputs": [],
   "source": [
    "#scale features \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T15:58:15.001245Z",
     "start_time": "2020-05-15T15:58:14.996193Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 3 available classifiers for Naive Bayes in sklearn.** \n",
    "\n",
    "1. Gaussian: It assumes that continuous features follow a normal distribution.\n",
    "![](https://www.simplypsychology.org/normal-distribution.jpg?ezimgfmt=rs:555x353/rscb18/ng:webp/ngcb18)\n",
    "\n",
    "2. Bernoulli: The binomial model is useful if your features are binary.\n",
    "\n",
    "3. Multinomial: It is useful if your features are discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T15:58:18.004301Z",
     "start_time": "2020-05-15T15:58:17.997774Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T15:58:19.841253Z",
     "start_time": "2020-05-15T15:58:19.830374Z"
    }
   },
   "outputs": [],
   "source": [
    "#make predictions and evaluate \n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "print('-'*40)\n",
    "print('Accuracy Score:')\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print('-'*40)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('-'*40)\n",
    "print('Classification Matrix:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons \n",
    "**Pros:** \n",
    "- It is not only a simple approach but also a fast and accurate method for prediction.\n",
    "- Naive Bayes has very low computation cost.\n",
    "- It can efficiently work on a large dataset.\n",
    "- It performs well in case of discrete response variable compared to the continuous variable.\n",
    "- It can be used with multiple class prediction problems.\n",
    "- It also performs well in the case of text analytics problems.\n",
    "- When the assumption of independence holds, a Naive Bayes classifier performs better compared to other models like logistic regression.\n",
    "\n",
    "**Cons:**\n",
    "- Require to remove correlated features because they are voted twice in the model and it can lead to over inflating importance.\n",
    "\n",
    "- If a categorical variable has a category in test data set which was not observed in training data set, then the model will assign a zero probability. It will not be able to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace smoothing. Sklearn applies Laplace smoothing by default when you train a Naive Bayes classifier. For more info on smoothing [go here](https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
