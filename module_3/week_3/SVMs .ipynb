{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple SVM\n",
    "\n",
    "In case of linearly separable data in two dimensions, as shown in Fig. 1, a typical machine learning algorithm tries to find a boundary that divides the data in such a way that the misclassification error can be minimized. If you closely look at Fig. 1, there can be several boundaries that correctly divide the data points. The two dashed lines as well as one solid line classify the data correctly.\n",
    "![](https://s3.amazonaws.com/stackabuse/media/implementing-svm-kernel-svm-python-scikit-learn-1.jpg)\n",
    "\n",
    "SVM differs from the other classification algorithms in the way that it chooses the decision boundary that maximizes the distance from the nearest data points of all the classes. An SVM doesn't merely find a decision boundary; it finds the most optimal decision boundary.\n",
    "\n",
    "The most optimal decision boundary is the one which has maximum margin from the nearest points of all the classes. The nearest points from the decision boundary that maximize the distance between the decision boundary and the points are called support vectors as seen in Fig 2. The decision boundary in case of support vector machines is called the maximum margin hyper plane.\n",
    "![](https://s3.amazonaws.com/stackabuse/media/implementing-svm-kernel-svm-python-scikit-learn-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linearly Separable Data \n",
    "In case of non-linearly separable data, the simple SVM algorithm cannot be used. Rather, a modified version of SVM, called Kernel SVM, is used.\n",
    "![](https://s3.amazonaws.com/stackabuse/media/implementing-svm-kernel-svm-python-scikit-learn-3.jpg)\n",
    "\n",
    "The kernel SVM projects the non-linearly separable data into lower dimensions to linearly separate data in higher dimensions in such a way that the data points belonging to different classes are allocated to different dimensions.\n",
    "\n",
    "Below, the data points are plotted on the x-axis and z-axis (Z is the squared sum of both x and y: z=x^2=y^2). Now you can easily segregate these points using linear separation.\n",
    "\n",
    "![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526288453/index_bnr4rx.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Kernel\n",
    "A linear kernel can be used just like we saw in linear regression, take the dot product of any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values.\n",
    "\n",
    "**K(x, xi) = sum(x * xi)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernal \n",
    "The polynomial kernel can distinguish curved or nonlinear input space. Where d is the degree of the polynomial. d=1 is similar to the linear transformation. The degree needs to be manually specified in the learning algorithm.\n",
    "\n",
    "**K(x,xi) = 1 + sum(x * xi)^d**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF - Radial Basis Function Kernel \n",
    "The Radial basis function kernel is a popular kernel function commonly used in support vector machine classification. RBF can map an input space in infinite dimensional space. Here **gamma** is a parameter, which ranges from 0 to 1. A higher value of gamma will perfectly fit the training dataset, which causes over-fitting. Gamma=0.1 is considered to be a good default value. The value of gamma needs to be manually specified in the learning algorithm.\n",
    "\n",
    "**K(x,xi) = exp(-gamma * sum((x – xi^2))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin and Kernel Tricks \n",
    "SVM addresses non-linearly separable cases by introducing two concepts: Soft Margin and Kernel Tricks.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*vwRojapdm0po85w8XnyWRQ.png)\n",
    "\n",
    "**2 Solutions:** \n",
    "\n",
    "1. Soft Margin: try to find a line to separate, but tolerate one or few misclassified dots (e.g. the dots circled in red)\n",
    "\n",
    "2. Kernel Trick: try to find a non-linear decision boundary\n",
    "\n",
    "### Soft Margin \n",
    "\n",
    "Two types of misclassifications are tolerated by SVM under **soft margin:**\n",
    "1. The dot is on the wrong side of the decision boundary but on the correct side/ on the margin (shown in left)\n",
    "\n",
    "2. The dot is on the wrong side of the decision boundary and on the wrong side of the margin (shown in right)\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*pNJ7IaXvSvxjpyUw3KKVzA.png)\n",
    "\n",
    "Applying Soft Margin, SVM tolerates a few dots to get misclassified and tries to balance the trade-off between finding a line that maximizes the margin and minimizes the misclassification.\n",
    "\n",
    "How much tolerance(soft) we want to give when finding the decision boundary is an important hyper-parameter for the SVM (both linear and nonlinear solutions). In Sklearn, it is represented as the penalty term — ‘C’. The bigger the C, the more penalty SVM gets when it makes misclassification. Therefore, the narrower the margin is and fewer support vectors the decision boundary will depend on.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*0vOVPBmYCkw-sUt77HtyGA.png)\n",
    "\n",
    "### Kernel Trick \n",
    "What Kernel Trick does is it utilizes existing features, applies some transformations, and creates new features. Those new features are the key for SVM to find the nonlinear decision boundary.\n",
    "In Sklearn — svm.SVC(), we can choose ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable as our kernel/transformation. I will give examples of the two most popular kernels — Polynomial and Radial Basis Function(RBF).\n",
    "![](https://miro.medium.com/max/1400/1*Ha7EfcfB5mY2RIKsXaTRkA.png)\n",
    "\n",
    "#### Polynomial Kernel\n",
    "Think of the polynomial kernel as a transformer/processor to generate new features by applying the polynomial combination of all the existing features.\n",
    "![](https://miro.medium.com/max/1400/1*gIHnZCcl4Q9fFx2AZsJ7pw.png)\n",
    "\n",
    "- Existing Feature: X = np.array([-2,-1,0, 1,2])\n",
    "- Label: Y = np.array([1,1,0,1,1])\n",
    "- it’s impossible for us to find a line to separate the yellow (1)and purple (0) dots (shown on the left).\n",
    "\n",
    "But, if we apply transformation X² to get:\n",
    "- New Feature: X = np.array([4,1,0, 1,4])\n",
    "- By combing the existing and new feature, we can certainly draw a line to separate the yellow purple dots (shown on the right).\n",
    "\n",
    "Support vector machine with a polynomial kernel can generate a non-linear decision boundary using those polynomial features.\n",
    "\n",
    "#### Radial Basis Function (RBF) kernel\n",
    "\n",
    "Think of the Radial Basis Function kernel as a transformer/processor to generate new features by measuring the distance between all other dots to a specific dot/dots — centers. The most popular/basic RBF kernel is the \n",
    "\n",
    "**Gaussian Radial Basis Function:**\n",
    "![](https://miro.medium.com/max/1260/1*izqr1xGcP89B7Xs1EluIQQ.png)\n",
    "\n",
    "**gamma (γ)** controls the influence of new features — Φ(x, center) on the decision boundary. The higher the gamma, the more influence of the features will have on the decision boundary, more wiggling the boundary will be.\n",
    "![](https://miro.medium.com/max/1400/1*M9spISHtIR_wOXKtmFTFvg.png)\n",
    "\n",
    "- Existing Feature: X = np.array([-2,-1,0, 1,2])\n",
    "- Label: Y = np.array([1,1,0,1,1])\n",
    "\n",
    "Again, it’s impossible for us to find a line to separate the dots (on left hand). But, if we apply Gaussian RBF transformation using two centers (-1,0) and (2,0) to get new features, we will then be able to draw a line to separate the yellow purple dots (on the right):\n",
    "\n",
    "- New Feature 1: X_new1 = array([1.01, 1.00, 1.01, 1.04, 1.09])\n",
    "- New Feature 2: X_new2 = array([1.09, 1.04, 1.01, 1.00, 1.01])\n",
    "\n",
    "By combining the soft margin (tolerance of misclassifications) and kernel trick together, SVMs are able to structure the decision boundary for linear non-separable cases.\n",
    "\n",
    "**Hyper-parameters like C or Gamma control how wiggling the SVM decision boundary could be.**\n",
    "- the higher the C, the more penalty SVM was given when it misclassified, and therefore the less wiggling the decision boundary will be\n",
    "\n",
    "- the higher the gamma, the more influence the feature data points will have on the decision boundary, thereby the more wiggling the boundary will be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T19:07:54.680608Z",
     "start_time": "2020-05-22T19:07:52.799518Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T19:08:03.000855Z",
     "start_time": "2020-05-22T19:08:02.615393Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "\n",
    "colnames = ['sepal-length', 'sepal-width', 'petal-length',\n",
    "            'petal-width', 'Class']\n",
    "\n",
    "iris = pd.read_csv(url, names=colnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T19:08:14.295060Z",
     "start_time": "2020-05-22T19:08:14.285862Z"
    }
   },
   "outputs": [],
   "source": [
    "X = iris.drop('Class', axis=1)\n",
    "y = iris['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T19:10:56.053249Z",
     "start_time": "2020-05-22T19:10:56.038522Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amberyandow/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#poly kernel must pass in degree \n",
    "clf = SVC(kernel='poly', degree=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T19:11:01.101028Z",
     "start_time": "2020-05-22T19:11:01.096325Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T19:11:04.345958Z",
     "start_time": "2020-05-22T19:11:04.334015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0]\n",
      " [ 0  8  0]\n",
      " [ 0  1  8]]\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        13\n",
      "Iris-versicolor       0.89      1.00      0.94         8\n",
      " Iris-virginica       1.00      0.89      0.94         9\n",
      "\n",
      "       accuracy                           0.97        30\n",
      "      macro avg       0.96      0.96      0.96        30\n",
      "   weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T19:14:31.258812Z",
     "start_time": "2020-05-22T19:14:31.249339Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amberyandow/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='sigmoid', max_iter=-1, probability=True, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rbf kernel \n",
    "clf = SVC(kernel='sigmoid', probability=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T19:14:33.084729Z",
     "start_time": "2020-05-22T19:14:33.081608Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T19:14:35.965356Z",
     "start_time": "2020-05-22T19:14:35.956452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 13  0]\n",
      " [ 0  8  0]\n",
      " [ 0  9  0]]\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       0.00      0.00      0.00        13\n",
      "Iris-versicolor       0.27      1.00      0.42         8\n",
      " Iris-virginica       0.00      0.00      0.00         9\n",
      "\n",
      "       accuracy                           0.27        30\n",
      "      macro avg       0.09      0.33      0.14        30\n",
      "   weighted avg       0.07      0.27      0.11        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amberyandow/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T17:02:29.537070Z",
     "start_time": "2020-05-22T17:02:29.534998Z"
    }
   },
   "outputs": [],
   "source": [
    "#try sigmoid kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros \n",
    "- Good for datasets with more variables than observations\n",
    "- Robust against outliers\n",
    "- Good performance\n",
    "- Good off-the-shelf model in general for several scenarios\n",
    "- Can approximate complex non-linear functions\n",
    "\n",
    "## Cons \n",
    "- Long training time required\n",
    "- Tuning required to determine optimal kernel for non-linear SVMs\n",
    "\n",
    "## Requirements\n",
    "- Scaled features\n",
    "- Null values filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
