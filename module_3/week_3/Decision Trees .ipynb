{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is like going to a doctor who asks a series of questions to determine the cause of your symptoms.\n",
    "\n",
    "**Let's Play 20 Questions** \n",
    "\n",
    "**What are some other real life examples(like our doctor) where our society uses a decision tree mentality to come to an answer?** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy and Information Gain \n",
    "The goal is to have our ultimate classes be fully \"ordered\" (for a binary dependent variable, we'd have the 1's in one group and the 0's in the other). So one way to assess the value of a split is to measure how disordered our groups are, and there is a notion of entropy that measures precisely this.\n",
    "\n",
    "The entropy of the whole dataset is given by:\n",
    "\n",
    "$\\large E = -\\Sigma^n_i p_i\\log_2(p_i)$,\n",
    "\n",
    "where $p_i$ is the probability of belonging to the $i$th group, where $n$ is the number of groups (i.e. target values).\n",
    "\n",
    "Entropy will always be between 0 and 1. The closer to 1, the more disordered your group.\n",
    "\n",
    "Let's use the math library's log() function to look at this:\n",
    "[Powerpoint presentation](https://docs.google.com/presentation/d/1kXs3Mi9a3w87J6tzs2sWyxW8kq2eaRQTBgUPKvuf8x8/edit#slide=id.p9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T17:23:06.067410Z",
     "start_time": "2020-05-20T17:23:06.061915Z"
    }
   },
   "outputs": [],
   "source": [
    "#if we start with a 50/50 distribution \n",
    "from math import log\n",
    "entropy = -0.5 * log(0.5, 2) - 0.5 * log(0.5, 2)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T17:26:10.232410Z",
     "start_time": "2020-05-20T17:26:10.227926Z"
    }
   },
   "outputs": [],
   "source": [
    "#tree will detect a feature that can split into 30/70\n",
    "entropy = -0.3 * log(0.3, 2) - 0.7 * log(0.7, 2)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given split, the information gain is simply the entropy of the parent group less the entropy of the split.\n",
    "\n",
    "For a given parent, then, we maximize our model's performance by minimizing the split's entropy.\n",
    "\n",
    "What we'd like to do then is:\n",
    "\n",
    "to look at the entropies of all possible splits, and\n",
    "to choose the split with the lowest entropy.\n",
    "In practice there are far too many splits for it to be practical for a person to calculate all these different entropies ...\n",
    "\n",
    "... but we can make computers do these calculations for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gini Impurity**\n",
    "\n",
    "An alternative metric to entropy comes from the work of Corrado Gini. The Gini Impurity is defined as:\n",
    "\n",
    "$\\large G = 1 - \\Sigma_ip_i^2$, or, equivalently, $\\large G = \\Sigma_ip_i(1-p_i)$.\n",
    "\n",
    "where, again, $p_i$ is the probability of belonging to the $i$th group.\n",
    "\n",
    "Gini Impurity will always be between 0 and 0.5. The closer to 0.5, the more disordered your group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T17:30:02.897535Z",
     "start_time": "2020-05-20T17:30:02.893360Z"
    }
   },
   "outputs": [],
   "source": [
    "#gini with 30/70 split \n",
    "1 - (0.7**2 + 0.3**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:58:13.599651Z",
     "start_time": "2020-05-18T20:58:13.595008Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "#df = pd.read_csv('/Users/amberyandow/Downloads/diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:26:44.996372Z",
     "start_time": "2020-05-18T20:26:44.986272Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:26:45.595038Z",
     "start_time": "2020-05-18T20:26:45.589212Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:26:46.230711Z",
     "start_time": "2020-05-18T20:26:46.116096Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(df['Outcome'],label=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:26:49.730498Z",
     "start_time": "2020-05-18T20:26:49.726599Z"
    }
   },
   "outputs": [],
   "source": [
    "#create numpy arrays for predictors and target variables \n",
    "X = df.drop('Outcome',axis=1).values\n",
    "y = df['Outcome'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:26:50.515032Z",
     "start_time": "2020-05-18T20:26:50.511660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:26:51.034303Z",
     "start_time": "2020-05-18T20:26:51.028443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:26:53.237731Z",
     "start_time": "2020-05-18T20:26:53.227241Z"
    }
   },
   "outputs": [],
   "source": [
    "print('-'*40)\n",
    "print('Accuracy Score:')\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print('-'*40)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('-'*40)\n",
    "print('Classification Matrix:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:17:00.326884Z",
     "start_time": "2020-05-18T20:17:00.323518Z"
    }
   },
   "outputs": [],
   "source": [
    "#brew install graphviz\n",
    "#pip install -U pydotplus\n",
    "\n",
    "#For installing graphviz on windows use the url below \n",
    "#https://bobswift.atlassian.net/wiki/spaces/GVIZ/pages/20971549/How+to+install+Graphviz+software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Tree \n",
    "\n",
    "**You will likely need to run the cell above before you can use the following code. Export_graphviz converts our classifier into a dot file and pydotplus converts the dot file in a png which can then be displayed into the notebook.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:27:16.769343Z",
     "start_time": "2020-05-18T20:26:57.832153Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "col_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
    "             'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = col_names,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('diabetes.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This unpruned tree is not very interpretable. Let's prune these trees and see if we get a better output.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "\n",
    "![](https://miro.medium.com/max/1136/1*3MDxpY_pIMs0yb4dc55KpQ.jpeg)\n",
    "\n",
    "[Great Medium Article that thoroughly goes over each hyperparameter](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:42:41.844476Z",
     "start_time": "2020-05-18T20:42:41.839470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:42:58.451647Z",
     "start_time": "2020-05-18T20:42:58.442209Z"
    }
   },
   "outputs": [],
   "source": [
    "print('-'*40)\n",
    "print('Accuracy Score:')\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print('-'*40)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('-'*40)\n",
    "print('Classification Matrix:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:44:38.464237Z",
     "start_time": "2020-05-18T20:44:37.554233Z"
    }
   },
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = col_names,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('diabetes.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T20:56:48.792514Z",
     "start_time": "2020-05-18T20:56:48.787269Z"
    }
   },
   "outputs": [],
   "source": [
    "top_feats = clf.feature_importances_\n",
    "top_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T21:00:47.799859Z",
     "start_time": "2020-05-18T21:00:47.689429Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# creating list of column names\n",
    "feat_names=list(col_names)\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(top_feats)[::-1]\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "names = [feat_names[i] for i in indices]\n",
    "\n",
    "# Create plot\n",
    "plt.figure()\n",
    "\n",
    "# Create plot title\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Add bars\n",
    "plt.bar(range(X_train.shape[1]), top_feats[indices])\n",
    "\n",
    "# Add feature names as x-axis labels\n",
    "plt.xticks(range(X_train.shape[1]), names, rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros & Cons \n",
    "[Scikit-Learn](https://scikit-learn.org/stable/modules/tree.html#tree)\n",
    "\n",
    "**Pros**\n",
    "- East to interpret \n",
    "- Easily capture non-linear patterns \n",
    "- No need to normalize columns \n",
    "- feature engineering/importance \n",
    "- Non-parametric/no assumptions \n",
    "\n",
    "**Cons**\n",
    "- Sensitive to noisy data(tends to overfit), can be reduced with tuning \n",
    "- Sensitive to variance, can be reduced by bagging/boosting \n",
    "- Biased when you have imbalanced data(can be fixed with smote) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
