{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines: Automating the Automatic Learning\n",
    "\n",
    "Pipelines can keep our code neat and clean all the way from gathering & cleaning our data, to creating models & fine-tuning them!\n",
    "\n",
    "**Advantages**: \n",
    "- Reduces complexity\n",
    "- Convenient \n",
    "- Flexible \n",
    "- Can help prevent mistakes(like data leakage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T18:19:58.208348Z",
     "start_time": "2020-05-27T18:19:57.100773Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer #replace missing data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T18:19:59.032348Z",
     "start_time": "2020-05-27T18:19:59.002299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting some data\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T18:23:26.155296Z",
     "start_time": "2020-05-27T18:23:26.138124Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amberyandow/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Define transformers (will adjust/massage the data)\n",
    "imputer = SimpleImputer(strategy=\"median\") # replaces missing values\n",
    "std_scaler = StandardScaler() # scales the data\n",
    "\n",
    "\n",
    "# Define the classifier (predictor) to train\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "# Have the classifer (and full pipeline) learn/train/fit from the data\n",
    "X_train_filled = imputer.fit_transform(X_train)\n",
    "X_train_scaled = std_scaler.fit_transform(X_train_filled)\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict using the trained classifier (still need to do the transformations)\n",
    "X_test_filled = imputer.transform(X_test)\n",
    "X_test_scaled = std_scaler.transform(X_test_filled)\n",
    "y_pred = rf_clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note that if we were to add more steps in this process, we'd have to change both the training and testing processes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T18:26:01.974368Z",
     "start_time": "2020-05-27T18:26:01.958108Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amberyandow/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")), \n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('rf_clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "# Train the pipeline (tranformations & predictor)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline (includes the transfomers & trained predictor)\n",
    "predicted = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper(\n",
    "    [(d, LabelEncoder()) for d in dummies]\n",
    ")\n",
    "\n",
    "lm = PMMLPipeline([(\"mapper\", mapper),\n",
    "                   (\"onehot\", OneHotEncoder()),\n",
    "                   (\"regressor\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of the Pipeline \n",
    "Scikit-learn has a class called [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that is very logical and versatile. We can break up the steps within a full process. But it'll help if we define what the different parts are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimators \n",
    "* This is any object in the pipeline that can can take in data and *estimate* (or **learn**) some parameters. \n",
    "\n",
    "This means regression and classification models are estimators but so are objects that transform the original dataset ([Transformers](pipeline_intro.ipynb#Transformer)) such as a standard scaling.\n",
    "\n",
    "* Fit()\n",
    "    - All estimators estimate/learn by calling the fit() method by passing in the dataset. Other parameters can be passed in to \"help\" the estimator to learn. These are called hyperparameters, parameters used to tweak the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "Some estimators can change the original data to something new, a **transformation**. You can think of examples of these **transformers** when you do scaling, data cleaning, or expanding/reducing on a dataset.\n",
    "\n",
    "* transform() \n",
    "    * Transformers will call the `transform()` method to apply the transformation to a dataset. Must do .fit() first. \n",
    "    \n",
    "* fir_transform()\n",
    "    * Remember that all estimators have a fit() method, so a transformer can use the fit() method to learn something about the given dataset. After learning with fit(), a transformation on the dataset can be made with the transform() method.\n",
    "\n",
    "    * An example of this would be a function that performs normalization on the dataset; the fit() method would learn the minimum and maximum of the dataset and the transform() method will scale the dataset.\n",
    "\n",
    "    * When you call fit and transform with the same dataset, you can simply call the fit_transform() method. This essentially has the same results as calling fit() and then transform() on the dataset but possibly with some optimization and efficiencies baked in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors \n",
    "We've been using predictors whenever we've been making predictions with a classifier or regressor. We would use the fit() method to train our predictor object and then feed in new data to make predictions (based on what it learned in the fitting stage).\n",
    "\n",
    "* predict()\n",
    "    * As you probably can guess, the predict() method predicts results from a dataset given to it after being trained with a fit() method\n",
    "    \n",
    "* score()\n",
    "    * Predictors also have a score() method that can be used to evaluate how well the predictor performed on a dataset (such as the test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "* Check out Aur√©lien Geron's notebook of an [end-to-end ml project](https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb) on his GitHub repo based around his book [_Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2nd ed)_](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)\n",
    "\n",
    "\n",
    "* [Using google colab for GPU/TPU speed](https://www.analyticsvidhya.com/blog/2020/03/google-colab-machine-learning-deep-learning/)\n",
    "\n",
    "[DataSchool video on cat feats for pipelines](https://www.dataschool.io/encoding-categorical-features-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
