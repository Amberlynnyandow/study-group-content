{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives \n",
    "* Understand what machine learning is and the different types of models \n",
    "* Understand more specifically what logistic regression is and how it's different from linear regression \n",
    "* Be able to explain the sigmoid function and how it is transforms our model \n",
    "* Be able to explain how we evaluate ML models going forward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wordstream-files-prod.s3.amazonaws.com/s3fs-public/styles/simple_image/public/images/machine-learning1.png?SnePeroHk5B9yZaLY7peFkULrfW8Gtaf&itok=yjEJbEKD)\n",
    "\n",
    "[Powerpoint](https://docs.google.com/presentation/d/1jdcJsWAmpwH0kAPW4UJNvBQrngyWn-ZIOmLV4qi4KgU/edit#slide=id.g54a549331a_0_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Logistic Regression? \n",
    "\n",
    "![](https://miro.medium.com/max/400/1*zLfpo6F_Bfi6uvRL6iLX_Q.jpeg)\n",
    "It belongs to a class of predictive models called _Generalized Linear Models_. All of these models have 2 things in common: They all define significant relationships between independent/dependent variables and they indicate the strength of the relationships. \n",
    "\n",
    "Different from Linear regression -- it can predict the probabilities associated with a success or a failure. Is this email likely spam? What is the probability that this citizen will vote Republican? Is this homeowner likely to default on their mortgage? Is this person likely to buy our product? Is this tumor likely to be cancerous or benign?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions \n",
    "**Logistic Regression Assumptions:**\n",
    "\n",
    "· Binary logistic regression requires the dependent variable to be binary.\n",
    "\n",
    "· Only the meaningful variables should be included.\n",
    "\n",
    "· The independent variables should be independent of each other. That is, the model should have little or no multi-collinearity.\n",
    "\n",
    "· The independent variables are linearly related to the log odds.\n",
    "\n",
    "· Logistic regression requires quite large sample sizes.\n",
    "\n",
    "**Key differences from Linear Regression:**\n",
    "* GLM does not assume a linear relationship between dependent and independent variables. However, it assumes a linear relationship between link function and independent variables in logit model.\n",
    "\n",
    "* The dependent variable need not to be normally distributed.\n",
    "\n",
    "* It does not uses OLS (Ordinary Least Square) for parameter estimation. Instead, it uses maximum likelihood estimation (MLE).\n",
    "\n",
    "* Errors need to be independent but not normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Equation\n",
    "![](https://miro.medium.com/max/571/0*tGVPGu3aa1rhTdfl.png)\n",
    "Let's say we've constructed our best-fit line, i.e. our linear predictor, $\\hat{L} = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$.\n",
    "\n",
    "Consider the following transformation:\n",
    "$\\large\\hat{y} = \\Large\\frac{1}{1 + e^{-\\hat{L}}} \\large= \\Large\\frac{1}{1 + e^{-(\\beta_0 + ... + \\beta_nx_n)}}$. This is called the sigmoid function.\n",
    "\n",
    "This function squeezes our predictions between 0 and 1. \n",
    "\n",
    "Suppose I'm building a model to predict whether a plant is poisonous or not, based perhaps on certain biological features of its leaves. I'll let '1' indicate a poisonous plant and '0' indicate a non-poisonous plant.\n",
    "\n",
    "Now I'm forcing my predictions to be between 0 and 1, so suppose for test plant $P$ I get some value like 0.19.\n",
    "\n",
    "I can naturally understand this as the probability that $P$ is poisonous.\n",
    "\n",
    "If I truly want a binary prediction, I can simply round my score appropriately.\n",
    "\n",
    "How do we fit a line to our dependent variable if its values are already stored as probabilities? We can use the inverse of the sigmoid function, and just set our regression equation equal to that. The inverse of the sigmoid function is called the logit function, and it looks like this:\n",
    "\n",
    "$\\large f(y) = \\ln\\left(\\frac{y}{1 - y}\\right)$. Notice that the domain of this function is $(0, 1)$.\n",
    "\n",
    "Quick proof that logit and sigmoid are inverse functions:\n",
    "\n",
    "$\\hspace{170mm}x = \\frac{1}{1 + e^{-y}}$;\n",
    "$\\hspace{170mm}$so $1 + e^{-y} = \\frac{1}{x}$;\n",
    "$\\hspace{170mm}$so $e^{-y} = \\frac{1 - x}{x}$;\n",
    "$\\hspace{170mm}$so $-y = \\ln\\left(\\frac{1 - x}{x}\\right)$;\n",
    "$\\hspace{170mm}$so $y = \\ln\\left(\\frac{x}{1 - x}\\right)$.)\n",
    "\n",
    "Our regression equation will now look like this:\n",
    "\n",
    "$\\large\\ln\\left(\\frac{y}{1 - y}\\right) = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classification Models \n",
    "\n",
    "For classification problems, the target is a categorical variable. This means that we can simply count the number of times that our model predicts the correct category and the number of times that it predicts something else.\n",
    "\n",
    "We can visualize this by means of a **confusion matrix**, a tabular representation of Actual vs Predicted values.\n",
    "![](https://miro.medium.com/max/350/0*rhntpf-9O0A5HjCP)\n",
    "\n",
    "**The metrics for evaluating your models performance can be drawn from this matrix** \n",
    "\n",
    "* Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "* Recall = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "* Precision = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "* F-1 Score = $\\frac{2PrRc}{Pr + Rc}$ = $\\frac{2TP}{2TP + FP + FN}$ \n",
    "\n",
    "**General Lessons**: \n",
    "First, let's make some general observations about the metrics we've so far defined.\n",
    "\n",
    "**Accuracy:**\n",
    "\n",
    "    * **Pro:** Takes into account both false positives and false negatives.\n",
    "\n",
    "    * **Con:** Can be misleadingly high when there is a significant class imbalance. (A lottery-ticket predictor that always predicts a loser will be highly accurate.)\n",
    "\n",
    "**Recall:**\n",
    "\n",
    "    * **Pro:** Highly sensitive to false negatives.\n",
    "\n",
    "    * **Con:** No sensitivity to false positives.\n",
    "\n",
    "**Precision:**\n",
    "\n",
    "    * **Pro:** Highly sensitive to false positives.\n",
    "\n",
    "    * **Con:** No sensitivity to false negatives.\n",
    "    \n",
    "    \n",
    "**PRACTICE DOCUMENT** - https://docs.google.com/document/d/1KkBBSRqaDXaHMoSNa9hrP0xipFtXoG5UEq7Mdon6RZ4/edit?usp=sharing\n",
    "\n",
    "**F-1 Score:**\n",
    "\n",
    "Harmonic mean of recall and precision.\n",
    "\n",
    "**AIC (Akaike Information Criteria**) — The analogous metric of adjusted R² in logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with minimum AIC value.\n",
    "\n",
    "**ROC Curve:** Receiver Operating Characteristic (ROC) summarizes the model’s performance by evaluating the trade-offs between true positive rate (sensitivity) and false positive rate (1- specificity). For plotting ROC, it is advisable to assume p > 0.5 since we are more concerned about success rate. ROC summarizes the predictive power for all possible values of p > 0.5. The area under curve (AUC), referred to as index of accuracy (A) or concordance index, is a perfect performance metric for ROC curve. Higher the area under curve, better the prediction power of the model. Below is a sample ROC curve. The ROC of a perfect predictive model has TP equals 1 and FP equals 0. This curve will touch the top left corner of the graph.\n",
    "![](https://miro.medium.com/max/300/0*20UWoOC5Gi4SdbAw.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
