{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"6\" color=\"scarlet\">Natural Language Processing</font>\n",
    "\n",
    "It is a field in machine learning/deep learning that deals with understanding, analyzing, manipulating and generating language. Humans communicate through language on multiple mediums these days. It gets complicated. There is context, intonation, inflection and body language. The first major advancement in machine language processing was in 1950 when Alan Turing published \"Computing Machinery and Intelligence\". This paper establsihed the Turing Test, a criterion for how well a computer could impersonate a human. In 1957, Noam Chomsky's paper on Syntactic Structures revolutionized our understanding of linguistics. But a few decades passed without any real progress. It wasn't until the late 80's when ML algorithms were introduced that NLP showed real promise.\n",
    "\n",
    " <font face=\"script\" size=\"4\">\"Learn a language and you'll avoid a war\"-Arab proverb</font>\n",
    "        \n",
    "\n",
    "_NLP is not Neuro-linguistic programming(pseuodo-science - think changing behavior through hypnosis). Natural Language Understanding is similar to NLP but a bit different. NLP focuses on turning unstructured data into structured data. NLU is focused on content or sentiment analysis._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">NLP in the Real World</font>\n",
    " \n",
    " Lots of everyday things we take for granted rely completely on NLP to function. Spell check and auto-complete, voice recognition/texting, spam filters, search engines, Siri/Alexa, google translate.\n",
    " \n",
    " - [AI having a convo](https://youtu.be/WnzlbyTZsQY)\n",
    " - [Summarize text](https://smmry.com/)\n",
    " - [Jennings vs. Watson](https://www.ted.com/talks/ken_jennings_watson_jeopardy_and_me_the_obsolete_know_it_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:28.296919Z",
     "start_time": "2020-12-08T18:51:26.044734Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:28.322266Z",
     "start_time": "2020-12-08T18:51:28.298634Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('job_scrape6.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:28.334715Z",
     "start_time": "2020-12-08T18:51:28.324706Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Preprocessing, Feature Engineering and EDA</font>\n",
    "* Casing \n",
    "* Punctuation \n",
    "* Stop word removal \n",
    "* Tokenization \n",
    "\n",
    "* Stemming \n",
    "* Lemmatization \n",
    "* POS tagging \n",
    "\n",
    "All of these are ways to help normalize our data, reduce randomness and dimensionality.\n",
    "\n",
    "→ Removal of duplicate whitespaces and punctuation.<br/>\n",
    "→ Accent removal (if your data includes diacritical marks from ‘foreign’ languages — this helps to reduce errors related to encoding type).<br/>\n",
    "→ Capital letter removal (often, working with lowercase words deliver better results. In some cases, however, capital letters are very important to extract information, like names and locations). <br/>\n",
    "→ Removal or substitution of special characters/emojis (e.g.: remove hashtags). <br/>\n",
    "→ Substitution of contractions (very common in English; e.g.: ‘I’m’→‘I am’). <br/>\n",
    "→ Transform word numerals into numbers (eg.: ‘twenty three’→‘23’). <br/>\n",
    "→ Substitution of values for their type (e.g.: ‘$50’→‘MONEY’). <br/>\n",
    "→ Acronym normalization (e.g.: ‘US’→‘United States’/‘U.S.A’) and abbreviation normalization (e.g.: ‘btw’→‘by the way’). <br/>\n",
    "→ Normalize date formats, social security numbers or other data that have a standard format. <br/>\n",
    "→ Spell correction (one could say that a word can be misspelled infinite ways, so spell corrections reduce the vocabulary variation by “correcting”) — this is very important if you’re dealing with open user inputs, such as tweets, IMs and emails. <br/>\n",
    "→ Removal of gender/time/grade variation with Stemming or Lemmatization. <br/>\n",
    "→ Substitution of rare words for more common synonyms. <br/>\n",
    "→ Stop word removal (more a dimensionality reduction technique than a normalization technique, but let us leave it here for the sake of mentioning it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:28.398246Z",
     "start_time": "2020-12-08T18:51:28.337168Z"
    }
   },
   "outputs": [],
   "source": [
    "#Getting rid of upper cases. This avoids having multiple copies of the same words \n",
    "df['lower_desc'] = df['description'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['lower_desc'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Regular Expressions</font>\n",
    "\n",
    "Regular expressions are specially encoded text strings used as patterns for matching sets of strings.\n",
    "![](regex_cheat_sheet.png)\n",
    "<a href=\"https://www.debuggex.com/cheatsheet/regex/python\">Regex Cheatsheet</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:28.438713Z",
     "start_time": "2020-12-08T18:51:28.399759Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing punctuation. It helps us reduce the size of the data \n",
    "df['lower_desc'] = df['lower_desc'].str.replace('[^\\w\\s]','')\n",
    "df['lower_desc'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\">**Stop Words Removal** - words that don't contribute to the significance or meaning of a document </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:28.445046Z",
     "start_time": "2020-12-08T18:51:28.440376Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "print(stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:28.452171Z",
     "start_time": "2020-12-08T18:51:28.446345Z"
    }
   },
   "outputs": [],
   "source": [
    "df['char_count'] = df['description'].str.len() #how many characters do we have in description? \n",
    "print(df[['description','char_count']].head())\n",
    "print(df['char_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:28.817252Z",
     "start_time": "2020-12-08T18:51:28.455044Z"
    }
   },
   "outputs": [],
   "source": [
    "#how many stop words do we have? \n",
    "df['stopwords'] = df['description'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "df[['description','stopwords']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:29.168350Z",
     "start_time": "2020-12-08T18:51:28.819984Z"
    }
   },
   "outputs": [],
   "source": [
    "#removing stopwords \n",
    "df['lower_desc'] = df['lower_desc'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df['lower_desc'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:29.221251Z",
     "start_time": "2020-12-08T18:51:29.170275Z"
    }
   },
   "outputs": [],
   "source": [
    "#most frequent and least frequent words \n",
    "freq = pd.Series(' '.join(df['lower_desc']).split()).value_counts()[:20]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:29.230898Z",
     "start_time": "2020-12-08T18:51:29.222709Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Tokenization</font>\n",
    "\n",
    "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:29.251933Z",
     "start_time": "2020-12-08T18:51:29.232354Z"
    }
   },
   "outputs": [],
   "source": [
    "desc_str = ' '.join(df['lower_desc'].tolist())\n",
    "print(desc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:29.976186Z",
     "start_time": "2020-12-08T18:51:29.253906Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(desc_str) #tokenizing \n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Stemming</font>\n",
    "- a technique to remove affixes from a word and ending up with the stem. Play would be the stem of a word and the 'ing' in playing would be an affix. This process makes similar words more equal to each other. This way the algorithm only has to learn the stem of the word instead of the stem and all its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:29.980575Z",
     "start_time": "2020-12-08T18:51:29.978012Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer \n",
    "porter = PorterStemmer() #instantiate\n",
    "lemma = WordNetLemmatizer() #instantiate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:29.985139Z",
     "start_time": "2020-12-08T18:51:29.982533Z"
    }
   },
   "outputs": [],
   "source": [
    "print(porter.stem(\"I studied physics\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Lemmatization</font>\n",
    "- similar to stemming but it brings context to the words with morphological(words relationships to other words) analysis. A lemma is the base form of all its inflectional forms. Inflections are added to the stem of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:42:12.233932Z",
     "start_time": "2020-12-08T22:42:12.231248Z"
    }
   },
   "outputs": [],
   "source": [
    "print(lemma.lemmatize(\"physics\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">POS Tagging</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:38.585036Z",
     "start_time": "2020-12-08T18:51:31.227334Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens_pos = nltk.pos_tag(tokens)\n",
    "pos_df = pd.DataFrame(tokens_pos, columns = ('word','POS'))\n",
    "pos_sum = pos_df.groupby('POS', as_index=False).count() # group by POS tags\n",
    "pos_sum.sort_values(['word'], ascending=[False]) # in descending order of number of words per tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:38.625258Z",
     "start_time": "2020-12-08T18:51:38.586428Z"
    }
   },
   "outputs": [],
   "source": [
    "#getting just the nouns\n",
    "filtered_pos = [ ]\n",
    "for one in tokens_pos:\n",
    "    if one[1] == 'NN' or one[1] == 'NNS' or one[1] == 'NNP' or one[1] == 'NNPS':\n",
    "        filtered_pos.append(one)\n",
    "print (len(filtered_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:38.705339Z",
     "start_time": "2020-12-08T18:51:38.627110Z"
    }
   },
   "outputs": [],
   "source": [
    "#the 100 most common nouns\n",
    "fdist_pos = nltk.FreqDist(filtered_pos)\n",
    "top_100_words = fdist_pos.most_common(100)\n",
    "print(top_100_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:38.716232Z",
     "start_time": "2020-12-08T18:51:38.707176Z"
    }
   },
   "outputs": [],
   "source": [
    "top_words_df = pd.DataFrame(top_100_words, columns = ('pos','count'))\n",
    "top_words_df['Word'] = top_words_df['pos'].apply(lambda x: x[0]) # split the tuple of POS\n",
    "top_words_df = top_words_df.drop('pos', 1) # drop the previous column\n",
    "top_words_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:39.803520Z",
     "start_time": "2020-12-08T18:51:38.717683Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,18))\n",
    "top_words_df.sort_values(by='count').plot.barh(x='Word',\n",
    "                      y='count',\n",
    "                      ax=ax,\n",
    "                      color=\"purple\")\n",
    "\n",
    "ax.set_title(\"Common Words Found in DS Job Descriptions(Without Stop Words)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:40.265838Z",
     "start_time": "2020-12-08T18:51:39.805188Z"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:40.272699Z",
     "start_time": "2020-12-08T18:51:40.269897Z"
    }
   },
   "outputs": [],
   "source": [
    "word_counts = ' '.join(top_words_df['Word'].tolist())\n",
    "print(type(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:51:40.566021Z",
     "start_time": "2020-12-08T18:51:40.274181Z"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(word_counts)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Modeling</font>\n",
    "\n",
    "### Naive Bayes Modeling\n",
    "\n",
    "Naive Bayes models lend themselves well to NLP problems. Consider the task of trying to predict genre from text. My subjective probability that a text belongs to a certain genre would be a function of the words in the text. So e.g. the (prior) probability that a text is science-fiction may be relatively small. But the probability that a text is science-fiction *given that it uses the word 'cyclotron'* may be quite high.\n",
    "\n",
    "### TF-IDF \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T17:41:57.056236Z",
     "start_time": "2020-06-16T17:41:57.052851Z"
    }
   },
   "source": [
    "<center><img src=\"tfidf.png\" height=600 width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:14:03.280547Z",
     "start_time": "2020-12-08T22:14:03.111242Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:05:36.900370Z",
     "start_time": "2020-12-08T22:05:36.889187Z"
    }
   },
   "outputs": [],
   "source": [
    "df['target'] = np.random.randint(0, 2, df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:10:59.732150Z",
     "start_time": "2020-12-08T22:10:59.728425Z"
    }
   },
   "outputs": [],
   "source": [
    "#setting our target & features \n",
    "X = df['lower_desc']\n",
    "y = df['target'] \n",
    "\n",
    "# generate a list of stopwords for TfidfVectorizer to ignore\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that takes in our various texts along with their respective labels and uses TF-IDF to vectorize the texts. Recall that TF-IDF helps us \"vectorize\" text (turn text into numbers) so we can do \"math\" with it. It is used to reflect how relevant a term is in a given document in a numerical way.\n",
    "\n",
    "This TF-IDF model rescales the values of important words and makes them comparable between each text in the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:12:10.920129Z",
     "start_time": "2020-12-08T22:12:10.916455Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate tf-idf vectorization (use sklearn's TfidfVectorizer) for our data\n",
    "def tfidf(X, y,  stopwords_list): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords_list)\n",
    "    tf_idf_train = vectorizer.fit_transform(X_train)\n",
    "    tf_idf_test = vectorizer.transform(X_test)\n",
    "    return tf_idf_train, tf_idf_test, y_train, y_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:14:48.094329Z",
     "start_time": "2020-12-08T22:14:47.925482Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf_train, tf_idf_test, y_train, y_test, vectorizer = tfidf(X, y, stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a set of vectorized training data we can use this data to train a classifier to learn how to classify a specific text based on the vectorized version of the text. The function below will accept a classifier object, a vectorized training set, vectorized test set, and list of training labels and return a list of predictions for our training set and a separate list of predictions for our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:15:59.109869Z",
     "start_time": "2020-12-08T22:15:59.107432Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:16:29.187315Z",
     "start_time": "2020-12-08T22:16:29.184149Z"
    }
   },
   "outputs": [],
   "source": [
    "# a function that takes in a classifier and trains it on our tf-idf vectors and generates test and train predictiions\n",
    "def classify_text(classifier, tf_idf_train, tf_idf_test, y_train):\n",
    "    classifier.fit(tf_idf_train, y_train)\n",
    "    train_preds = classifier.predict(tf_idf_train)\n",
    "    test_preds = classifier.predict(tf_idf_test)\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:16:44.557813Z",
     "start_time": "2020-12-08T22:16:44.548032Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate predictions with Naive Bayes Classifier\n",
    "nb_train_preds, nb_test_preds = classify_text(nb_classifier, tf_idf_train, tf_idf_test, y_train)\n",
    "\n",
    "# evaluate performance of Naive Bayes Classifier\n",
    "print(confusion_matrix(y_test, nb_test_preds))\n",
    "print(accuracy_score(y_test, nb_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:17:05.282315Z",
     "start_time": "2020-12-08T22:17:03.701631Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate predictions with Random Forest Classifier\n",
    "rf_train_preds, rf_test_preds = classify_text(rf_classifier, tf_idf_train, tf_idf_test, y_train)\n",
    "\n",
    "# evaluate performance of Random Forest Classifier\n",
    "print(confusion_matrix(y_test, rf_test_preds))\n",
    "print(accuracy_score(y_test, rf_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "$\\begin{align}\n",
    "idf(w) = \\log \\dfrac{N}{df_t}\n",
    "\\end{align} $\n",
    "\n",
    "Let's figure out which words are the most important to each class of texts! Recall that Inverse Document Frequency can help us determine which words are most important in an entire corpus or group of documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:40:06.302426Z",
     "start_time": "2020-12-08T22:40:06.299097Z"
    }
   },
   "outputs": [],
   "source": [
    "#function that calculates the inverse document frequency(IDF) of each word in our collection\n",
    "def get_idf(class_, df, stopwords_list):\n",
    "    docs = df[df.target==class_].lower_desc\n",
    "    class_dict = {} \n",
    "    for doc in docs:\n",
    "        words = set(doc.split())\n",
    "        for word in words:\n",
    "            if word.lower() not in stopwords_list: \n",
    "                class_dict[word.lower()] = class_dict.get(word.lower(), 0) + 1\n",
    "    idf_df = pd.DataFrame.from_dict(class_dict, orient='index')\n",
    "    idf_df.columns = ['IDF']\n",
    "    idf_df.IDF = np.log(len(docs)/idf_df.IDF)\n",
    "    idf_df = idf_df.sort_values(by=\"IDF\", ascending=True)\n",
    "    return idf_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:24:17.588722Z",
     "start_time": "2020-12-08T22:24:17.416762Z"
    }
   },
   "outputs": [],
   "source": [
    "get_idf(0 , df, stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Resources</font>\n",
    "* [Text blob library](https://textblob.readthedocs.io/en/dev/api_reference.html#textblob.blob.Word)\n",
    "\n",
    "* [Googles n-gram viewer](https://books.google.com/ngrams/graph?content=API&year_start=1800&year_end=2010&corpus=0&smoothing=3&direct_url=t1%3B%2CAPI%3B%2Cc0)\n",
    "\n",
    "* [Tweepy - python library for accessing the Twitter API](https://www.tweepy.org/)\n",
    "\n",
    "* [Step by step guide for NLP](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
