{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Unsupervised Learning \n",
    "It's all about clustering PCA. \n",
    "\n",
    "Goals: \n",
    "* Understand what unsupervised learning is \n",
    "* Assess what scenarios could use $k$-means\n",
    "* Articulate the methodology used by $k$-means\n",
    "* Apply KMeans from sklearn.cluster to a relevant dataset\n",
    "* Select the appropriate number of clusters using $k$-means and the elbow method\n",
    "* Evaluate the weaknesses and remedies to $k$-means\n",
    "* Apply Principal component analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning \n",
    "No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\n",
    "![](https://learn.g2.com/hs-fs/hubfs/unsupervised-learning.png?width=700&name=unsupervised-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality\n",
    "[Learn.co - Curse of Dimensionality](https://learn.co/tracks/module-4-data-science-career-2-1/big-data-deep-learning-and-natural-language-processing/section-33-principal-component-analysis/curse-of-dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Methods\n",
    "[Clustering_Notebook](https://github.com/Amberlynnyandow/clustering_seattle-ds/blob/master/kmeans-flatiron_edited-by-gd.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis\n",
    "\n",
    "Think of the predictors in your dataset as dimensions in what we can usefully call \"feature space\". If we're predicting house prices, then we might have a 'square feet' dimension or a 'number of bathrooms' dimension, etc. Then each record (of a house or a house sale, say) would be represented as a point (or vector) in this feature space. Some would score higher on the 'latitude' dimension or lower on the 'number of bedrooms' dimension. But are these variables truly independent of each other? When I increase my position along the 'number of bedrooms' dimension--or, better, _direction_, I also tend to increase my position along, say, the 'square feet' direction as well. [Visualize](https://awwapp.com/#)\n",
    "\n",
    "The idea behind PCA is to transform our dataset into something more useful for building models. What we want to do is to build new dimensions (predictors) out of the dimensions we are given in such a way that:\n",
    "\n",
    "- each dimension we draw captures as much of the remaining variance among our predictors as possible; and\n",
    "- each dimension we draw is orthogonal to the ones we've already drawn.\n",
    "\n",
    "There are four steps to conducting PCA:\n",
    "1. Center each feature by subtracting the feature mean\n",
    "2. Calculate the covariance matrix(a square matrix giving the covariance between each pair of elements of a given random vector) for your normalized dataset\n",
    "3. Calculate the eigenvectors/eigenvalues for the covariance matrix(The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.)\n",
    "4. Reorder your eigenvectors based on their accompanying eigenvalues (in descending order of the eigenvalues)\n",
    "5. Take the dot product of the transpose of the eigenvectors with the transpose of the normalized data - [Refresher on Dot Product](https://www.mathsisfun.com/algebra/vectors-dot-product.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:25:18.065396Z",
     "start_time": "2020-06-02T15:25:16.483472Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:26:02.992793Z",
     "start_time": "2020-06-02T15:26:02.980722Z"
    }
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:26:50.062012Z",
     "start_time": "2020-06-02T15:26:50.059285Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on fitting and transforming\n",
    "Fitting finds the internal parameters of a model that will be used to transform data. Transforming applies the parameters to data. You may fit a model to one set of data, and then transform it on a completely different set.\n",
    "\n",
    "For example, you fit a linear model to data to get a slope and intercept. Then you use those parameters to transform (i.e., map) new or existing values of x to y. OR You fit data to find the principal components. Then you transform your data to see how it maps onto these components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:27:01.306297Z",
     "start_time": "2020-06-02T15:27:01.297592Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:27:03.764392Z",
     "start_time": "2020-06-02T15:27:03.753077Z"
    }
   },
   "outputs": [],
   "source": [
    "scaled_data = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:27:24.466859Z",
     "start_time": "2020-06-02T15:27:24.401962Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:11:11.655761Z",
     "start_time": "2020-06-02T16:11:11.645237Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA().fit(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:28:12.898055Z",
     "start_time": "2020-06-02T15:28:12.895132Z"
    }
   },
   "outputs": [],
   "source": [
    "x_pca = pca.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:28:21.278948Z",
     "start_time": "2020-06-02T15:28:21.274779Z"
    }
   },
   "outputs": [],
   "source": [
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:28:29.422431Z",
     "start_time": "2020-06-02T15:28:29.418780Z"
    }
   },
   "outputs": [],
   "source": [
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:28:41.199886Z",
     "start_time": "2020-06-02T15:28:41.041494Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='plasma')\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:11:19.741619Z",
     "start_time": "2020-06-02T16:11:19.627527Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the components\n",
    "\n",
    "The components correspond to combinations of the original features, the components themselves are stored as an attribute of the fitted PCA object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:29:55.818291Z",
     "start_time": "2020-06-02T15:29:55.813492Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this numpy matrix array, each row represents a principal component, and each column relates back to the original features. we can visualize this relationship with a heatmap. \n",
    "\n",
    "This heatmap and the color bar basically represent the correlation between the various feature and the principal component itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T16:22:56.193929Z",
     "start_time": "2020-06-04T16:22:56.184975Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comp = pd.DataFrame(pca.components_,columns=cancer['feature_names'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:31:02.240297Z",
     "start_time": "2020-06-02T15:31:02.002176Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df_comp,cmap='plasma',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVIEW\n",
    "* **Unsupervised learning** is used when there are no labels in your data. The 2 most common unsupervised learning tasks are clustering and dimensionality reduction. \n",
    "\n",
    "* **K-means** is a non-hierarchical clustering technique with 2 assumptions: \n",
    "\n",
    "    1. To compute the \"cluster center\", you calculate the (arithmetic) mean of all the points belonging to the cluster. Each cluster center is recalculated in the beginning of each new iteration\n",
    "    2. After the cluster center has been recalculated, if a given point is now closer to a different cluster center than the center of its current cluster, then that point is reassigned to the clostest center\n",
    "\n",
    "* **PCA** is a method that brings together:\n",
    "\n",
    "    1. A measure of how each variable is associated with one another. (Covariance matrix.)\n",
    "    2. The directions in which our data are dispersed. (Eigenvectors.)\n",
    "    3. The relative importance of these different directions. (Eigenvalues.)\n",
    "    \n",
    "PCA combines our predictors and allows us to drop the eigenvectors that are relatively unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
