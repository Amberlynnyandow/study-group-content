{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"6\" color=\"scarlet\">Natural Language Processing</font>\n",
    "\n",
    "It is a field in machine learning/deep learning that deals with understanding, analyzing, manipulating and generating language. Humans communicate through language on multiple mediums these days. It gets complicated. There is context, intonation, inflection and body language. The first major advancement in machine language processing was in 1950 when Alan Turing published \"Computing Machinery and Intelligence\". This paper establsihed the Turing Test, a criterion for how well a computer could impersonate a human. In 1957, Noam Chomsky's paper on Syntactic Structures revolutionized our understanding of linguistics. But a few decades passed without any real progress. It wasn't until the late 80's when ML algorithms were introduced that NLP showed real promise.\n",
    "\n",
    " <font face=\"script\" size=\"4\">\"Learn a language and you'll avoid a war\"-Arab proverb</font>\n",
    "        \n",
    "\n",
    "_NLP is not Neuro-linguistic programming(pseuodo-science - think changing behavior through hypnosis). Natural Language Understanding is similar to NLP but a bit different. NLP focuses on turning unstructured data into structured data. NLU is focused on content or sentiment analysis._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">NLP in the Real World</font>\n",
    " \n",
    " Lots of everyday things we take for granted rely completely on NLP to function. Spell check and auto-complete, voice recognition/texting, spam filters, search engines, Siri/Alexa, google translate.\n",
    " \n",
    " - [AI having a convo](https://youtu.be/WnzlbyTZsQY)\n",
    " - [Summarize text](https://smmry.com/)\n",
    " - [Jennings vs. Watson](https://www.ted.com/talks/ken_jennings_watson_jeopardy_and_me_the_obsolete_know_it_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T15:52:47.584026Z",
     "start_time": "2020-06-16T15:52:45.379586Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T15:58:47.908276Z",
     "start_time": "2020-06-16T15:58:47.877239Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('job_scrape6.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T15:58:56.924739Z",
     "start_time": "2020-06-16T15:58:56.913172Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Preprocessing, Feature Engineering and EDA</font>\n",
    "* Casing \n",
    "* Punctuation \n",
    "* Stop word removal \n",
    "* Tokenization \n",
    "\n",
    "* Stemming \n",
    "* Lemmatization \n",
    "* POS tagging \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Regular Expressions</font>\n",
    "![](regex_cheat_sheet.png)\n",
    "<a href=\"https://www.debuggex.com/cheatsheet/regex/python\">Regex Cheatsheet</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:06:07.547874Z",
     "start_time": "2020-06-16T16:06:07.486175Z"
    }
   },
   "outputs": [],
   "source": [
    "#Getting rid of upper cases. This avoids having multiple copies of the same words \n",
    "df['lower_desc'] = df['description'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['lower_desc'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:06:16.034368Z",
     "start_time": "2020-06-16T16:06:15.988424Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing punctuation. It helps us reduce the size of the data \n",
    "df['lower_desc'] = df['lower_desc'].str.replace('[^\\w\\s]','')\n",
    "df['lower_desc'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\">**Stop Words Removal** - words that don't contribute to the significance or meaning of a document </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:07:39.654066Z",
     "start_time": "2020-06-16T16:07:39.648497Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:08:01.651766Z",
     "start_time": "2020-06-16T16:08:01.639413Z"
    }
   },
   "outputs": [],
   "source": [
    "df['char_count'] = df['description'].str.len() #how many characters do we have in description? \n",
    "df[['description','char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:09:02.951874Z",
     "start_time": "2020-06-16T16:09:02.609146Z"
    }
   },
   "outputs": [],
   "source": [
    "#how many stop words do we have? \n",
    "df['stopwords'] = df['description'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "df[['description','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:09:23.028660Z",
     "start_time": "2020-06-16T16:09:22.695145Z"
    }
   },
   "outputs": [],
   "source": [
    "#removing stopwords \n",
    "df['lower_desc'] = df['lower_desc'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df['lower_desc'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:09:38.610811Z",
     "start_time": "2020-06-16T16:09:38.552758Z"
    }
   },
   "outputs": [],
   "source": [
    "#most frequent and least frequent words \n",
    "freq = pd.Series(' '.join(df['lower_desc']).split()).value_counts()[:20]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:11:19.741146Z",
     "start_time": "2020-06-16T16:11:19.730840Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Tokenization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:15:45.593185Z",
     "start_time": "2020-06-16T16:15:45.568683Z"
    }
   },
   "outputs": [],
   "source": [
    "desc_str = ' '.join(df['lower_desc'].tolist())\n",
    "print(desc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:16:06.724303Z",
     "start_time": "2020-06-16T16:16:06.043038Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(desc_str) #tokenizing \n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Stemming</font>\n",
    "- a technique to remove affixes from a word and ending up with the stem. Play would be the stem of a word and the 'ing' in playing would be an affix. This process makes similar words more equal to each other. This way the algorithm only has to learn the stem of the word instead of the stem and all its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:18:21.278638Z",
     "start_time": "2020-06-16T16:18:21.275755Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer \n",
    "porter = PorterStemmer() #instantiate\n",
    "lemma = WordNetLemmatizer() #instantiate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(porter.stem(\"I studied physics\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Lemmatization</font>\n",
    "- similar to stemming but it brings context to the words with morphological(words relationships to other words) analysis. A lemma is the base form of all its inflectional forms. Inflections are added to the stem of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemma.lemmatize(\"physics\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">POS Tagging</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:28:24.453837Z",
     "start_time": "2020-06-16T16:28:17.857012Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens_pos = nltk.pos_tag(tokens)\n",
    "pos_df = pd.DataFrame(tokens_pos, columns = ('word','POS'))\n",
    "pos_sum = pos_df.groupby('POS', as_index=False).count() # group by POS tags\n",
    "pos_sum.sort_values(['word'], ascending=[False]) # in descending order of number of words per tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:29:03.070075Z",
     "start_time": "2020-06-16T16:28:55.990229Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens_pos = nltk.pos_tag(tokens)\n",
    "pos_df = pd.DataFrame(tokens_pos, columns = ('word','POS'))\n",
    "pos_sum = pos_df.groupby('POS', as_index=False).count() # group by POS tags\n",
    "pos_sum.sort_values(['word'], ascending=[False]) # in descending order of number of words per tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:30:17.837245Z",
     "start_time": "2020-06-16T16:30:17.790798Z"
    }
   },
   "outputs": [],
   "source": [
    "#getting just the nouns\n",
    "filtered_pos = [ ]\n",
    "for one in tokens_pos:\n",
    "    if one[1] == 'NN' or one[1] == 'NNS' or one[1] == 'NNP' or one[1] == 'NNPS':\n",
    "        filtered_pos.append(one)\n",
    "print (len(filtered_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:30:18.725003Z",
     "start_time": "2020-06-16T16:30:18.647948Z"
    }
   },
   "outputs": [],
   "source": [
    "#the 100 most common nouns\n",
    "fdist_pos = nltk.FreqDist(filtered_pos)\n",
    "top_100_words = fdist_pos.most_common(100)\n",
    "print(top_100_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:31:12.893464Z",
     "start_time": "2020-06-16T16:31:12.878507Z"
    }
   },
   "outputs": [],
   "source": [
    "top_words_df = pd.DataFrame(top_100_words, columns = ('pos','count'))\n",
    "top_words_df['Word'] = top_words_df['pos'].apply(lambda x: x[0]) # split the tuple of POS\n",
    "top_words_df = top_words_df.drop('pos', 1) # drop the previous column\n",
    "top_words_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:31:55.668839Z",
     "start_time": "2020-06-16T16:31:54.736018Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,18))\n",
    "top_words_df.sort_values(by='count').plot.barh(x='Word',\n",
    "                      y='count',\n",
    "                      ax=ax,\n",
    "                      color=\"purple\")\n",
    "\n",
    "ax.set_title(\"Common Words Found in DS Job Descriptions(Without Stop Words)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:36:35.887036Z",
     "start_time": "2020-06-16T16:36:35.833110Z"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:42:16.551186Z",
     "start_time": "2020-06-16T16:42:16.547662Z"
    }
   },
   "outputs": [],
   "source": [
    "word_counts = ' '.join(top_words_df['Word'].tolist())\n",
    "print(type(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:42:31.570340Z",
     "start_time": "2020-06-16T16:42:31.265703Z"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(word_counts)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T17:41:57.056236Z",
     "start_time": "2020-06-16T17:41:57.052851Z"
    }
   },
   "source": [
    "<center><img src=\"tfidf.png\" height=600 width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:50:20.382377Z",
     "start_time": "2020-06-16T16:50:20.380174Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T17:09:23.243469Z",
     "start_time": "2020-06-16T17:09:23.238595Z"
    }
   },
   "outputs": [],
   "source": [
    "df['lower_desc'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform text into a bag of words. CountVectorizer learns from the text with the fit method and then transforms the text into a list of lists(a matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T17:09:44.200614Z",
     "start_time": "2020-06-16T17:09:43.948297Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "vectorizer = text.CountVectorizer().fit(df.lower_desc)\n",
    "vectorized_text = vectorizer.transform(df.lower_desc)\n",
    "print(vectorized_text.todense()[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must normalize the text length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T17:09:49.087600Z",
     "start_time": "2020-06-16T17:09:49.079593Z"
    }
   },
   "outputs": [],
   "source": [
    "TfidF = text.TfidfTransformer(norm='l1')\n",
    "tfidf = TfidF.fit_transform(vectorized_text)\n",
    "\n",
    "phrase = 3 #choose 0-3\n",
    "total = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This TF-IDF model rescales the values of important words and makes them comparable between each text in the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T17:13:33.828059Z",
     "start_time": "2020-06-16T17:13:33.816730Z"
    }
   },
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T17:11:05.794530Z",
     "start_time": "2020-06-16T17:09:55.484552Z"
    }
   },
   "outputs": [],
   "source": [
    "for word in vectorizer.vocabulary_:\n",
    "    pos = vectorizer.vocabulary_[word]\n",
    "    value = list(tfidf.toarray()[phrase])[pos]\n",
    "    if value !=0:\n",
    "        print(\"%10s: %0.3f\" % (word, value))\n",
    "        total += value\n",
    "print('\\nSummed values of a phrase: %0.1f' % total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">N-Grams</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:33:28.500462Z",
     "start_time": "2020-06-16T16:33:24.503752Z"
    }
   },
   "outputs": [],
   "source": [
    "TextBlob(desc_str).ngrams(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Resources</font>\n",
    "* [Text blob library](https://textblob.readthedocs.io/en/dev/api_reference.html#textblob.blob.Word)\n",
    "\n",
    "* [Googles n-gram viewer](https://books.google.com/ngrams/graph?content=API&year_start=1800&year_end=2010&corpus=0&smoothing=3&direct_url=t1%3B%2CAPI%3B%2Cc0)\n",
    "\n",
    "* [Tweepy - python library for accessing the Twitter API](https://www.tweepy.org/)\n",
    "\n",
    "* [Step by step guide for NLP](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
