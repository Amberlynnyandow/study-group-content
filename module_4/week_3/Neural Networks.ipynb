{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Neural Networks \n",
    "\n",
    "#### Objectives: \n",
    "- Compare NN to regression \n",
    "- Understand the role of each peice in a network \n",
    "- Pros and cons of the different activation functions \n",
    "- Introduce hyperparameters and backpropagation \n",
    "- Look at ways to add complexity and avoid overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember when we went from linear to logistic regression?\n",
    "![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1523361626/linear_vs_logistic_regression_h8voek.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used a linear combination of variables\n",
    "\n",
    "$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2, x_2 +\\ldots + \\hat\\beta_n x_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And passed the sum product of those variables and coefficients through a sigmoid function.\n",
    "\n",
    "$$ P(y) = \\displaystyle \\frac{1}{1+e^{-(\\hat y)}}$$\n",
    "\n",
    "![](https://mathworld.wolfram.com/images/eps-gif/SigmoidFunction_701.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way of writing this:\n",
    "![log-reg-der](log_reg_deriv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we change the orientation of the first part, we get a new diagram:\n",
    "![](https://miro.medium.com/max/1280/1*8VSBCaqL2XeSCZQe_BAyVA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more general notation for a single layer NN:\n",
    "![fnn](First_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic Scenario \n",
    "A perceptron basically allows us to separate data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:04:40.559863Z",
     "start_time": "2020-06-17T17:04:39.749626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some initial setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:04:40.889128Z",
     "start_time": "2020-06-17T17:04:40.884876Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_logic(inputs, correct_outputs, weights, bias):\n",
    "    our_outputs = []\n",
    "    w1,w2 = weights\n",
    "    # Check each output\n",
    "    for test_input, correct_output in zip(inputs, correct_outputs):\n",
    "        linear_combination = w1 * test_input[0] + w1 * test_input[1] + bias\n",
    "        output = int(linear_combination >= 0)\n",
    "        is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "        our_outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "    # Print results\n",
    "    num_wrong = len([output[4] for output in our_outputs if output[4] == 'No'])\n",
    "    output_frame = pd.DataFrame(\n",
    "                        our_outputs, \n",
    "                        columns=[\n",
    "                            'Input 1', \n",
    "                            '  Input 2', \n",
    "                            '  Linear Combination', \n",
    "                            '  Activation Output', \n",
    "                            '  Is Correct']\n",
    "    )\n",
    "\n",
    "    print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical `AND`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:05:46.649430Z",
     "start_time": "2020-06-17T17:05:46.645633Z"
    }
   },
   "source": [
    "$A \\land B = C$\n",
    "\n",
    "Truth table:\n",
    "\n",
    "| A | B | C |\n",
    "|---|---|---|\n",
    "| 1 | 1 | 1 |\n",
    "| 1 | 0 | 0 |\n",
    "| 0 | 1 | 0 |\n",
    "| 0 | 0 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "(x1) ⇒ (w1) ⇒\n",
    "        \n",
    "(x2) ⇒ (w2) ⇒    ⇒  (linear combination)  ⇒ (>= 0 ?) ⇒ (Output)\n",
    "\n",
    "(b)  ⇒ (1)  ⇒  \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:07:01.824016Z",
     "start_time": "2020-06-17T17:07:01.821102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:07:09.263888Z",
     "start_time": "2020-06-17T17:07:09.261036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set your values\n",
    "w1 = 0.0\n",
    "w2 = 0.0\n",
    "bias = 0.0\n",
    "\n",
    "weights = (w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:07:18.966102Z",
     "start_time": "2020-06-17T17:07:18.957417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test it out!\n",
    "test_logic(test_inputs, correct_outputs, weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of a Neural Network\n",
    "\n",
    "### Layers \n",
    "\n",
    "- **Input Layer**: the initial parameters (these will be the parts we feed to our network)\n",
    "- **Output Layer**: the classification (or regression predictions)\n",
    "- **Hidden Layer(s)**: the other neurons potentially in a neural network to find more complex patterns\n",
    "\n",
    "### Weights\n",
    "\n",
    ">The weights from our inputs are describing how much they should contribute to the next neuron. A larger weight means this neuron matters more. Negative weights mean the neuron has the opposite effect(friend who hates movies you like).\n",
    "\n",
    "But we can also think of the weights of hidden layer neurons telling us how much of these linear separations should be combined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:15:27.953513Z",
     "start_time": "2020-06-17T17:15:27.950188Z"
    }
   },
   "outputs": [],
   "source": [
    "def arctan(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        return 1/(1+np.square(x))\n",
    "    return np.arctan(x)\n",
    "\n",
    "z = np.arange(-10,10,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid \n",
    "Range: $(0,1)$\n",
    "\n",
    "Function: $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "**Advantages**:\n",
    "- Relatively intuitive at classifications\n",
    "- Commonly used\n",
    "\n",
    "**Disadvantages**:\n",
    "- Not as efficient at training(doesn't work with backprop)\n",
    "- vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:17:32.537008Z",
     "start_time": "2020-06-17T17:17:32.407360Z"
    }
   },
   "outputs": [],
   "source": [
    "#linear combinations on x-axis, blue line gradient\n",
    "def sigmoid(x, derivative=False):\n",
    "    f = 1 / (1 + np.exp(-x))\n",
    "    if (derivative == True):\n",
    "        return f * (1 - f)\n",
    "    return f\n",
    "\n",
    "y = sigmoid(z)\n",
    "dy = sigmoid(z, derivative=True)\n",
    "plt.title(\"sigmoid\")\n",
    "plt.axhline(color=\"gray\", linewidth=1,)\n",
    "plt.axvline(color=\"gray\", linewidth=1,)\n",
    "plt.plot(z, y, 'r')\n",
    "plt.plot(z, dy, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ## Hyperbolic Tangent - Tanh\n",
    "Range: $(-1,1)$\n",
    "\n",
    "Function: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "\n",
    "**Advantages**:\n",
    "- More efficient at training than sigmoid\n",
    "- Steeper gradient\n",
    "\n",
    "**Disadvantages**:\n",
    "- Still suffers from the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:19:30.981513Z",
     "start_time": "2020-06-17T17:19:30.853884Z"
    }
   },
   "outputs": [],
   "source": [
    "def tanh(x, derivative=False):\n",
    "    f = np.tanh(x)\n",
    "    if (derivative == True):\n",
    "        return (1 - (f ** 2))\n",
    "    return np.tanh(x)\n",
    "\n",
    "y = tanh(z)\n",
    "dy = tanh(z, derivative=True)\n",
    "plt.title(\"sigmoid\")\n",
    "plt.axhline(color=\"gray\", linewidth=1,)\n",
    "plt.axvline(color=\"gray\", linewidth=1,)\n",
    "plt.plot(z, y, 'r')\n",
    "plt.plot(z, dy, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLu \n",
    "Range: $(0,\\infty)$\n",
    "\n",
    "Function: $f(x) = \n",
    "    \\begin{cases}\n",
    "      0, & \\text{if}\\ x<0 \\\\\n",
    "      x, & \\text{if}\\ x\\ge 0\n",
    "    \\end{cases}$\n",
    "    \n",
    "**Advantages**:\n",
    "- Calculation is relatively efficient\n",
    "- Specify a more positive activation\n",
    "\n",
    "**Disadvantages**:\n",
    "- Zero value: longer to train. Can't tell the difference between a little negative and very negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:21:09.298457Z",
     "start_time": "2020-06-17T17:21:09.181561Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(x, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = 0\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = 0\n",
    "    return f\n",
    "\n",
    "plt.title(\"ReLU\")\n",
    "y = relu(z)\n",
    "dy = relu(z, derivative=True)\n",
    "plt.axhline(color=\"gray\", linewidth=1,)\n",
    "plt.axvline(color=\"gray\", linewidth=1,)\n",
    "plt.plot(z, dy, 'b')\n",
    "plt.plot(z, y, 'r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky ReLu\n",
    "Range: $(-\\infty,\\infty)$\n",
    "\n",
    "Function: $f(x) = \n",
    "    \\begin{cases}\n",
    "      - c \\cdot x, & \\text{if}\\ x<0 \\\\\n",
    "      x, & \\text{if}\\ x\\ge 0\n",
    "    \\end{cases}\\  \\text{where}\\ c\\ \\text{is some small value (0.01)}$\n",
    "    \n",
    "**Advantages**:\n",
    "- Helps with training speed\n",
    "\n",
    "**Disadvantages**:\n",
    "- Still has to compute the slope when x is negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:22:38.649466Z",
     "start_time": "2020-06-17T17:22:38.531901Z"
    }
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, leakage = 0.05, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = leakage\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = x[i]* leakage\n",
    "    return f\n",
    "\n",
    "# the default leakage here is 0.05!\n",
    "y = leaky_relu(z)\n",
    "dy = leaky_relu(z, derivative=True)\n",
    "plt.axhline(color=\"gray\", linewidth=1,)\n",
    "plt.axvline(color=\"gray\", linewidth=1,)\n",
    "plt.title(\"leaky ReLU\")\n",
    "plt.xlim(-10,10)\n",
    "plt.plot(z, y, 'r')\n",
    "plt.plot(z, dy, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Hyperparameters \n",
    "\n",
    "- **Learning Rate ($\\alpha$)**: how big of a step we take in gradient descent\n",
    "- **Number of epochs**: how many times we repeat this process\n",
    "- **batch-size**: how many data points we use in a single training session (1 epoch)\n",
    "\n",
    "Remember, any parameter adjusted to enhance the neural network's learning _is_ a hyperparameter (this includes the actual structure of the neural net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation \n",
    "The **backpropagation** algorithm takes the idea of optimally adjusting the parameters (weights) to get a better result. \n",
    "\n",
    "We do this tuning by propogating the (average) error back through the network, with the cost function $J$ guiding us and adjusting via gradient descent.\n",
    "\n",
    "> Turn down previous neurons that give a bad result\n",
    ">\n",
    "> Turn up previous neurons that give a good result\n",
    "\n",
    "> Great video explanation of backpropogation by 3Blue1Brown (part of a full playlist): [Backpropagation calculus | Deep learning, chapter 4](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:28:04.155975Z",
     "start_time": "2020-06-17T17:28:04.042832Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(40)\n",
    "y = np.random.rand(40)\n",
    "z = (x + y) < np.random.rand(40)*2\n",
    "\n",
    "plt.scatter(x,y,c=z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add More Layers but remember\n",
    "\n",
    "> More complexity can increase our chances of overfitting\n",
    ">\n",
    "> More parameters mean more computation (takes longer to train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Train-validation-test sets\n",
    "\n",
    "- Think of training as what you study for a test\n",
    "- Think of validation is using a practice test (note sometimes called dev)\n",
    "- Think of testing as what you use to judge the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Model Complexity Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:33:07.160579Z",
     "start_time": "2020-06-17T17:33:07.033372Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "validation_error = np.array([5,3.5,2,3,4])\n",
    "train_error = np.array([4.5,3,1.5,1,0.5])\n",
    "n_epochs = np.array([5,50,100,200,300])\n",
    "\n",
    "plt.scatter(n_epochs, train_error,)\n",
    "plt.scatter(n_epochs, validation_error)\n",
    "plt.legend(['train error','validation error'])\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping \n",
    "Not the best solution but there are 2 options that are better. \n",
    "\n",
    "We train our model but only keep the best model it comes across. We can do this with a [ModelCheckpoint callback](https://keras.io/callbacks/#modelcheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:34:29.672137Z",
     "start_time": "2020-06-17T17:34:27.950019Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(\"best_model.h5\",\n",
    "                                             save_best_only=True\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "# Now points to the best model found during the fit\n",
    "model = keras.models.load_model(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also stop our training early when our test error isn't really changing. We can do this with a [EarlyStopping callback](https://keras.io/callbacks/#earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:37:48.338482Z",
     "start_time": "2020-06-17T17:37:48.327822Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "\n",
    "checkpoint = callbacks.EarlyStopping(monitor='val_loss', # What to watch\n",
    "                                     min_delta=0.01, # How much change to get\n",
    "                                     patience=5 # No change after 5 epochs\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too much Confidence in your model \n",
    "\n",
    "When a model has large weights, the model is \"too confident\"\n",
    "\n",
    "We need to punish large (confident) weights by contributing them to the error function\n",
    "\n",
    "### L1 Regularization - Absolute Value\n",
    "\n",
    "- Tend to get sparse vectors (small weights go to 0)\n",
    "- Reduce number of weights\n",
    "- Good feature selection to pick out importance\n",
    "\n",
    "$$ J(W,b) = -\\dfrac{1}{m} \\sum^m_{i=1}\\big[\\mathcal{L}(\\hat y_i, y_i)+ \\dfrac{\\lambda}{m}|w_i| \\big]$$\n",
    "\n",
    "### L2 Regularization - Squared Value\n",
    "\n",
    "- Not sparse vectors (weights homogeneous & small)\n",
    "- Tends to give better results for training\n",
    "\n",
    "    \n",
    "$$ J(W,b) = -\\dfrac{1}{m} \\sum^m_{i=1}\\big[\\mathcal{L}(\\hat y_i, y_i)+ \\dfrac{\\lambda}{m}w_i^2 \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout \n",
    "\n",
    "### Avoiding the Self-Perpetuating Strength Training\n",
    "\n",
    "When working out, we'd train our left and right arms evenly and switch our exercise routine throughout the week.\n",
    "\n",
    "In neural networks, we switch around which nodes we use during our training.\n",
    "\n",
    "Assign a probability of using a given node for that epoch (usually about 20% chance). When we have many epochs, we likely will even out the randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Dense(128, input_dim=100, activation='relu', name='input_layer'))\n",
    "model.add(Dropout(0.2, name='input_dropout'))\n",
    "# Hidden Layer\n",
    "model.add(Dense(256, activation='relu', name='hidden_layer1'))\n",
    "model.add(Dropout(0.2, name='hidden_layer1_dropout'))\n",
    "# Output Layer\n",
    "model.add(Dense(n_classes, activation='softmax', name='output'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice and Play \n",
    "- [playground.tensorflow.org](https://playground.tensorflow.org): A visual playground for us to train a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
