{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives \n",
    "1. Intro to Unsupervised Learning \n",
    "2. Intro Kmeans Algorithm \n",
    "3. Take a look at several ways to visualize your kmeans clustering algorithm:\n",
    "    - Clusters \n",
    "    - Elbow method \n",
    "    - Yellowbrick \n",
    "    - Silhouette\n",
    "    \n",
    "4. Hierarchical and Density based Clustering\n",
    "5. Color Compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning \n",
    "\n",
    "No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\n",
    "![](https://learn.g2.com/hs-fs/hubfs/unsupervised-learning.png?width=700&name=unsupervised-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering \n",
    "\n",
    "K-means is a partitional clustering algorithm - it divides data objects into non-overlapping groups. No object can be a member of more than 1 cluster and each cluster must have at least 1 object. These techniques require the user to specify the number of clusters, indicated by the variable k. Many partitional clustering algorithms work through an iterative process to assign subsets of data points into k clusters. \n",
    "\n",
    "These algorithms are both **nondeterministic**, meaning they could produce different results from two separate runs even if the runs were based on the same input.\n",
    "\n",
    "Partitional clustering methods have several strengths:\n",
    "\n",
    "- They work well when clusters have a spherical shape.\n",
    "- They’re scalable with respect to algorithm complexity.\n",
    "\n",
    "They also have weaknesses:\n",
    "\n",
    "- They’re not well suited for clusters with complex shapes and different sizes.\n",
    "- They break down when used with clusters of different densities.\n",
    "\n",
    "**What are the 2 main objectives of a k-means algorithm?** \n",
    "\n",
    "- Similarity amoung clusters \n",
    "- dissimilarity among other clusters \n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*dcWOeMRD1JMAoe7O9Ien0Q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T17:37:33.094745Z",
     "start_time": "2020-11-19T17:37:32.618302Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Create Clusters\n",
    "Generate a distribution of 8 clusters with 250 samples and plot them as a scatterplot. How many clusters do you recognize with your eye. Try to change the cluster standard deviation cluster_std until it will be hard for you to discriminate the 8 different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T17:38:03.593857Z",
     "start_time": "2020-11-19T17:38:03.451273Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=250, \n",
    "                  n_features=8, \n",
    "                  centers=8, \n",
    "                  cluster_std=0.5, \n",
    "                  shuffle=True, \n",
    "                  random_state=0)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], \n",
    "             marker='o', edgecolor='black', s=50)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) `KMeans`\n",
    "Import the method `KMeans` from `sklearn.cluster`. Instantiate a model km with 8 clusters(`n_clusters=8`). Set the maximum number of iterations to `max_iter=300` and `n_init=10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T21:13:02.531230Z",
     "start_time": "2020-11-19T21:13:02.515111Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=8, \n",
    "            init='random', \n",
    "            n_init=10, \n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X)\n",
    "km.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Display the clustered data\n",
    "\n",
    "Use the function `PlotClusters` to display the clustered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T17:48:55.180207Z",
     "start_time": "2020-11-19T17:48:55.174277Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import colors as mcolors\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "ColorNames=list(colors.keys())\n",
    "HSV=colors.values()\n",
    "\n",
    "\n",
    "def PlotClusters(X,y, km):\n",
    "    \n",
    "    for ClusterNumber in range(km.n_clusters):\n",
    "        plt.scatter(X[y_km == ClusterNumber, 0],\n",
    "                X[y_km == ClusterNumber, 1],\n",
    "                s=50, c=ColorNames[ClusterNumber+1],\n",
    "                marker='s', edgecolor='black',\n",
    "                label='cluster {0}'.format(ClusterNumber+1))\n",
    "    plt.scatter(km.cluster_centers_[:, 0],\n",
    "        km.cluster_centers_[:, 1],\n",
    "        s=250, marker='*',\n",
    "        c='red', edgecolor='black',\n",
    "        label='centroids')\n",
    "    plt.legend(scatterpoints=1)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('images/11_02.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Variation of the number k of clusters \n",
    "\n",
    "Vary the number of clusters `n_clusters=8` in your `KMeans` clustering algorithm from 4 to 8 and display each time the result using the function `PlotClusters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T17:48:57.538411Z",
     "start_time": "2020-11-19T17:48:56.810917Z"
    }
   },
   "outputs": [],
   "source": [
    "for n_clusters in range(4,9):\n",
    "    km = KMeans(n_clusters=n_clusters,\n",
    "                init='random', \n",
    "                n_init=10, \n",
    "                max_iter=300,\n",
    "                tol=1e-04,\n",
    "                random_state=0)\n",
    "\n",
    "    y_km = km.fit_predict(X)\n",
    "    PlotClusters(X,y, km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the elbow method to find the optimal number of clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T17:49:00.093397Z",
     "start_time": "2020-11-19T17:49:00.089570Z"
    }
   },
   "outputs": [],
   "source": [
    "#Sum of squared distances of samples to their closest cluster center - euclidean distance\n",
    "print('Distortion: %.2f' % km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T17:49:01.763765Z",
     "start_time": "2020-11-19T17:49:01.394582Z"
    }
   },
   "outputs": [],
   "source": [
    "distortions = []\n",
    "ScoreList   = []\n",
    "maxNumberOfClusters=15\n",
    "\n",
    "for i in range(1, maxNumberOfClusters):\n",
    "    km = KMeans(n_clusters=i, \n",
    "                init='k-means++', \n",
    "                n_init=10, \n",
    "                max_iter=300, \n",
    "                random_state=0)\n",
    "    km.fit(X)\n",
    "    distortions.append(km.inertia_)\n",
    "    ScoreList.append(-km.score(X))\n",
    "    \n",
    "    \n",
    "plt.plot(range(1, maxNumberOfClusters), distortions, marker='o')\n",
    "plt.plot(range(1, maxNumberOfClusters), ScoreList, marker='^')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "#plt.savefig('images/11_03.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T17:49:05.028735Z",
     "start_time": "2020-11-19T17:49:03.049641Z"
    }
   },
   "outputs": [],
   "source": [
    "##  install yellowbrck library -- pip install yellowbrick\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "\n",
    "visualizer = KElbowVisualizer(model, k=(1,12), timings=False)\n",
    "\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.show()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T17:49:05.266087Z",
     "start_time": "2020-11-19T17:49:05.030868Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer\n",
    "# The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion.\n",
    "\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(\n",
    "    model, k=(2,10), metric='calinski_harabasz', timings=False\n",
    ")\n",
    "\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.show()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sklearn documention on calinski_harabasz score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying the quality of clustering  via silhouette plots\n",
    "\n",
    "- Silhouette score for a set of sample data points is used to measure how dense and well-separated the clusters are.\n",
    "- Silhouette score takes into consideration the intra-cluster distance between the sample and other data points within the same cluster (a) and inter-cluster distance between the sample and the next nearest cluster (b).\n",
    "- The silhouette score falls within the range [-1, 1].\n",
    "- The silhouette score of 1 means that the clusters are very dense and nicely separated. The score of 0 means that clusters are overlapping. The score of less than 0 means that data belonging to clusters may be wrong/incorrect.\n",
    "- The silhouette plots can be used to select the most optimal value of the K (no. of cluster) in K-means clustering.\n",
    "- The aspects to look out for in Silhouette plots are cluster scores below the average silhouette score, wide fluctuations in the size of the clusters, and also the thickness of the silhouette plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T17:49:24.110637Z",
     "start_time": "2020-11-19T17:49:23.591988Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "km = KMeans(n_clusters=8, \n",
    "            init='k-means++', \n",
    "            n_init=10, \n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "cluster_labels = np.unique(y_km)\n",
    "n_clusters = cluster_labels.shape[0]\n",
    "silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')\n",
    "y_ax_lower, y_ax_upper = 0, 0\n",
    "yticks = []\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "    c_silhouette_vals.sort()\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(float(i) / n_clusters)\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n",
    "             edgecolor='none', color=color)\n",
    "\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "    \n",
    "silhouette_avg = np.mean(silhouette_vals)\n",
    "plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n",
    "\n",
    "plt.yticks(yticks, cluster_labels + 1)\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Silhouette coefficient')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_04.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locating regions of high density via [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) and [Heirarchical Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T18:45:15.955368Z",
     "start_time": "2020-06-09T18:45:15.810602Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_14.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering determines cluster assignments by building a hierarchy. This is implemented by either a bottom-up or a top-down approach:\n",
    "\n",
    "- **Agglomerative clustering** is the bottom-up approach. It merges the two points that are the most similar until all points have been merged into a single cluster.\n",
    "\n",
    "- **Divisive clustering** is the top-down approach. It starts with all points as one cluster and splits the least similar clusters at each step until only single data points remain.\n",
    "\n",
    "These methods produce a tree-based hierarchy of points called a **dendrogram**. Similar to partitional clustering, in hierarchical clustering the number of clusters (k) is often predetermined by the user. Clusters are assigned by cutting the dendrogram at a specified depth that results in k groups of smaller dendrograms.\n",
    "\n",
    "Unlike many partitional clustering techniques, hierarchical clustering is a deterministic process, meaning cluster assignments won’t change when you run an algorithm twice on the same input data.\n",
    "\n",
    "The **strengths** of hierarchical clustering methods include the following:\n",
    "\n",
    "- They often reveal the finer details about the relationships between data objects.\n",
    "- They provide an interpretable dendrogram.\n",
    "\n",
    "The **weaknesses** of hierarchical clustering methods include the following:\n",
    "\n",
    "- They’re computationally expensive with respect to algorithm complexity.\n",
    "- They’re sensitive to noise and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T18:45:40.861461Z",
     "start_time": "2020-06-09T18:45:40.545887Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "km = KMeans(n_clusters=2, random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "ax1.scatter(X[y_km == 0, 0], X[y_km == 0, 1],\n",
    "            edgecolor='black',\n",
    "            c='lightblue', marker='o', s=40, label='cluster 1')\n",
    "ax1.scatter(X[y_km == 1, 0], X[y_km == 1, 1],\n",
    "            edgecolor='black',\n",
    "            c='red', marker='s', s=40, label='cluster 2')\n",
    "ax1.set_title('K-means clustering')\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=2,\n",
    "                             affinity='euclidean',\n",
    "                             linkage='complete')\n",
    "y_ac = ac.fit_predict(X)\n",
    "ax2.scatter(X[y_ac == 0, 0], X[y_ac == 0, 1], c='lightblue',\n",
    "            edgecolor='black',\n",
    "            marker='o', s=40, label='cluster 1')\n",
    "ax2.scatter(X[y_ac == 1, 0], X[y_ac == 1, 1], c='red',\n",
    "            edgecolor='black',\n",
    "            marker='s', s=40, label='cluster 2')\n",
    "ax2.set_title('Agglomerative clustering')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/11_15.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-Based Clustering\n",
    "\n",
    "**Density-based clustering** determines cluster assignments based on the density of data points in a region. Clusters are assigned where there are high densities of data points separated by low-density regions.\n",
    "\n",
    "Unlike the other clustering categories, this approach doesn’t require the user to specify the number of clusters. Instead, there is a distance-based parameter that acts as a tunable threshold. This threshold determines how close points must be to be considered a cluster member.\n",
    "\n",
    "Examples of density-based clustering algorithms include Density-Based Spatial Clustering of Applications with Noise, or DBSCAN, and Ordering Points To Identify the Clustering Structure, or OPTICS.\n",
    "\n",
    "The **strengths** of density-based clustering methods include the following:\n",
    "\n",
    "- They excel at identifying clusters of nonspherical shapes.\n",
    "- They’re resistant to outliers.\n",
    "\n",
    "The **weaknesses** of density-based clustering methods include the following:\n",
    "\n",
    "- They aren’t well suited for clustering in high-dimensional spaces.\n",
    "- They have trouble identifying clusters of varying densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T18:51:02.322241Z",
     "start_time": "2020-06-09T18:51:02.155423Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.2, min_samples=5, metric='euclidean')\n",
    "y_db = db.fit_predict(X)\n",
    "plt.scatter(X[y_db == 0, 0], X[y_db == 0, 1],\n",
    "            c='lightblue', marker='o', s=40,\n",
    "            edgecolor='black', \n",
    "            label='cluster 1')\n",
    "plt.scatter(X[y_db == 1, 0], X[y_db == 1, 1],\n",
    "            c='red', marker='s', s=40,\n",
    "            edgecolor='black', \n",
    "            label='cluster 2')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_16.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An interesting application of clustering: Color compression\n",
    "One interesting application of clustering is supposed to be color image compression. For example, imagine you have an image with millions of colors. In most images, a large number of the colors will be unused, and conversely a large number of pixels will have similar or identical colors. Clustering can help find Ncolor clusters in the data, and we can use this to create a new image where the true input color is replaced by the color of the closest cluster.\n",
    "\n",
    "**Why does Image Segmentation even matter?**\n",
    "\n",
    "If we take an example of Autonomous Vehicles, they need sensory input devices like cameras, radar, and lasers to allow the car to perceive the world around it, creating a digital map. Autonomous driving is not even possible without object detection which itself involves image classification/segmentation.\n",
    "\n",
    "![](https://miro.medium.com/max/1000/1*GbFzGi-QWi28vawIxOZxfg.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T18:58:37.473785Z",
     "start_time": "2020-06-09T18:58:37.248948Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "flower = load_sample_image(\"flower.jpg\")\n",
    "ax = plt.axes(xticks=[], yticks=[])\n",
    "ax.imshow(flower);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image is made up of several intensity values known as Pixels. In a colored image, each pixel is of 3 bytes containing RGB (Red-Blue-Green) values having Red intensity value, then Blue and then Green intensity value for each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T18:59:00.707612Z",
     "start_time": "2020-06-09T18:59:00.700811Z"
    }
   },
   "outputs": [],
   "source": [
    "print(flower.shape)\n",
    "data = flower / 255.0 # use 0...1 scale\n",
    "data = data.reshape(427 * 640, 3)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T18:59:44.449158Z",
     "start_time": "2020-06-09T18:59:43.442376Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pixels(data, title, colors=None, N=10000):\n",
    "    if colors is None:\n",
    "        colors = data\n",
    "    # choose a random subset\n",
    "    rng = np.random.RandomState(0)\n",
    "    i = rng.permutation(data.shape[0])[:N]\n",
    "    colors = colors[i]\n",
    "    R, G, B = data[i].T\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    ax[0].scatter(R, G, color=colors, marker='.')\n",
    "    ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "    ax[1].scatter(R, B, color=colors, marker='.')\n",
    "    ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "    fig.suptitle(title, size=20);\n",
    "    \n",
    "plot_pixels(data, title='Input color space: 16 million possible colors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T19:00:20.040009Z",
     "start_time": "2020-06-09T19:00:17.739153Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')  \n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans = MiniBatchKMeans(4)\n",
    "kmeans.fit(data)\n",
    "new_colors = kmeans.cluster_centers_[kmeans.predict(data)]\n",
    "\n",
    "plot_pixels(data, colors=new_colors,\n",
    "            title=\"Reduced color space: 16 colors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T19:00:59.072371Z",
     "start_time": "2020-06-09T19:00:58.832199Z"
    }
   },
   "outputs": [],
   "source": [
    "flower_recolored = new_colors.reshape(flower.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6),\n",
    "                       subplot_kw=dict(xticks=[], yticks=[]))\n",
    "fig.subplots_adjust(wspace=0.05)\n",
    "ax[0].imshow(flower)\n",
    "ax[0].set_title('Original Image', size=16)\n",
    "ax[1].imshow(flower_recolored)\n",
    "ax[1].set_title('16-color Image', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[For more info on using clustering for color compression](https://towardsdatascience.com/image-compression-using-k-means-clustering-aa0c91bb0eeb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
