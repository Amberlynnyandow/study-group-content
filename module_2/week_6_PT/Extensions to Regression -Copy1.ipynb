{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Extensions-to-Regression\" data-toc-modified-id=\"Extensions-to-Regression-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Extensions to Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Log-Transformations\" data-toc-modified-id=\"Log-Transformations-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Log Transformations</a></span></li><li><span><a href=\"#Feature-Scaling-&amp;-Normalization\" data-toc-modified-id=\"Feature-Scaling-&amp;-Normalization-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Feature Scaling &amp; Normalization</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Different-ways-to-scale\" data-toc-modified-id=\"Different-ways-to-scale-1.2.0.1\"><span class=\"toc-item-num\">1.2.0.1&nbsp;&nbsp;</span>Different ways to scale</a></span></li><li><span><a href=\"#Should-I-normalize/scale-or-standardize?\" data-toc-modified-id=\"Should-I-normalize/scale-or-standardize?-1.2.0.2\"><span class=\"toc-item-num\">1.2.0.2&nbsp;&nbsp;</span>Should I normalize/scale or standardize?</a></span></li></ul></li></ul></li><li><span><a href=\"#Polynomials\" data-toc-modified-id=\"Polynomials-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Polynomials</a></span></li><li><span><a href=\"#Bias-Variance-Trade-Off\" data-toc-modified-id=\"Bias-Variance-Trade-Off-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Bias Variance Trade-Off</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions to Regression \n",
    "* Log Transformations \n",
    "* Feature Scaling & Normalizing & Standardization \n",
    "* Polynomials \n",
    "* Bias & Variance \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transformations \n",
    "We use log transformations in linear regression to make skewed or exponential distributions more normal.\n",
    "\n",
    "A regression model will have unit changes between the x and y variables, where a single unit change in x will coincide with a constant change in y. Taking the log of one or both variables will effectively change the case from a unit change to a percent change.\n",
    "\n",
    "A logarithm is the base of a positive number. For example, the base10 log of 100 is 2, because 10^2 = 100. So the natural log function and the exponential function (ex) are inverses of each other.\n",
    "\n",
    "Keynote: 0.1 unit change in log(x) is equivalent to 10% increase in X.\n",
    "\n",
    "[Learn.co Lesson does a good job explaining](https://learn.co/tracks/module-2-data-science-career-2-1/statistics-ab-testing-and-linear-regression/section-19-multiple-regression-and-model-validation/log-transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to have all of our variables be on the same scale, so that the resulting coefficients are easier to interpret. If the scales of the variables are very different one from another, then some of the coefficients may end up on very large or very tiny scales.\n",
    "\n",
    "#### Different ways to scale\n",
    "\n",
    "Mean normalization: the distribution will have values between -1 and 1, and a mean of 0.\n",
    "$$x' = \\dfrac{x - \\text{mean}(x)}{\\max(x)-\\min(x)}$$\n",
    "\n",
    "Min-max scaling: brings values between 0 and 1\n",
    "$$x' = \\dfrac{x - \\min(x)}{\\max(x)-\\min(x)}$$\n",
    "\n",
    "Standardization: Does not help with normality. It changes the mean to 0 and the SD to 1\n",
    "$$x' = \\dfrac{x - \\bar x}{\\sigma}$$\n",
    "\n",
    "x' will have mean $\\mu = 0$ and $\\sigma = 1$\n",
    "\n",
    "#### Should I normalize/scale or standardize? \n",
    "\n",
    "* Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n",
    "\n",
    "\n",
    "* Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n",
    "\n",
    "\n",
    "\n",
    "When does the scale of the features matter for linear regression?\n",
    "Let's say that TV was measured in dollars, rather than thousands of dollars. How would that affect the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T01:23:30.564850Z",
     "start_time": "2020-11-17T01:23:28.950287Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Advertising.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2bfeafbbe210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Advertising.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_adv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Radio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_adv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sales'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'Advertising.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#import and upload our advertising DS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "df_adv = pd.read_csv('Advertising.csv')\n",
    "X = df_adv[['TV', 'Radio']]\n",
    "y = df_adv['Sales']\n",
    "df_adv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T01:23:30.567355Z",
     "start_time": "2020-11-17T01:23:28.951Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X = df_adv[['TV', 'Radio']]\n",
    "y = df_adv['Sales']\n",
    "\n",
    "## fit a OLS model with intercept on TV and Radio\n",
    "X = sm.add_constant(X)\n",
    "est = sm.OLS(y, X).fit()\n",
    "\n",
    "## Create the 3d plot -- skip reading this\n",
    "# TV/Radio grid for 3d plot\n",
    "xx1, xx2 = np.meshgrid(np.linspace(X.TV.min(), X.TV.max(), 100),\n",
    "np.linspace(X.Radio.min(), X.Radio.max(), 100))\n",
    "# plot the hyperplane by evaluating the parameters on the grid\n",
    "Z = est.params[0] + est.params[1] * xx1 + est.params[2] * xx2\n",
    "\n",
    "# create matplotlib 3d axes\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = Axes3D(fig, azim=-115, elev=15)\n",
    "\n",
    "# plot hyperplane\n",
    "surf = ax.plot_surface(xx1, xx2, Z, cmap=plt.cm.RdBu_r, alpha=0.6, linewidth=0)\n",
    "\n",
    "# plot data points - points over the HP are white, points below are black\n",
    "resid = y - est.predict(X)\n",
    "ax.scatter(X[resid >= 0].TV, X[resid >= 0].Radio, y[resid >= 0], color='black', alpha=1.0, facecolor='white')\n",
    "ax.scatter(X[resid < 0].TV, X[resid < 0].Radio, y[resid < 0], color='black', alpha=1.0)\n",
    "\n",
    "# set axis labels\n",
    "ax.set_xlabel('TV')\n",
    "ax.set_ylabel('Radio')\n",
    "ax.set_zlabel('Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T01:23:30.568634Z",
     "start_time": "2020-11-17T01:23:28.952Z"
    }
   },
   "outputs": [],
   "source": [
    "#sklearn \n",
    "# create X and y\n",
    "feature_cols = ['TV']\n",
    "X = df_adv[feature_cols]\n",
    "y = df_adv.Sales\n",
    "\n",
    "# instantiate and fit\n",
    "mlr = LinearRegression()\n",
    "mlr.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print(mlr.intercept_)\n",
    "print(mlr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T01:23:30.570747Z",
     "start_time": "2020-11-17T01:23:28.954Z"
    }
   },
   "outputs": [],
   "source": [
    "# manually calculate the prediction\n",
    "7.0326 + 0.0475*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T01:23:30.572566Z",
     "start_time": "2020-11-17T01:23:28.956Z"
    }
   },
   "outputs": [],
   "source": [
    "#create new column \n",
    "df_adv['TV_dollars'] = df_adv.TV * 1000\n",
    "df_adv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that there was a new market where the TV advertising spend was $50,000. What would we predict for the Sales in that market?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T01:23:30.574155Z",
     "start_time": "2020-11-17T01:23:28.958Z"
    }
   },
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV_dollars']\n",
    "X = df_adv[feature_cols]\n",
    "y = df_adv.Sales\n",
    "\n",
    "# instantiate and fit\n",
    "mlr = LinearRegression()\n",
    "mlr.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print(mlr.intercept_)\n",
    "print(mlr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the TV_dollars coefficient (β1)?\n",
    "\n",
    "* A \"unit\" increase in TV ad spending is associated with a 0.0000475 \"unit\" increase in Sales.\n",
    "* Meaning: An additional dollar spent on TV ads is associated with an increase in sales of 0.0475 widgets.\n",
    "* Meaning: An additional $1,000 spent on TV ads is associated with an increase in sales of 47.5 widgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomials \n",
    "\n",
    "A very popular non-linear regression technique is Polynomial Regression, a technique which models the relationship between the response and the predictors as an n-th order polynomial. The higher the order of the polynomial the more \"wigglier\" functions you can fit. Using higher order polynomial comes at a price, however. First, the computational complexity of model fitting grows as the number of adaptable parameters grows. Second, more complex models have a higher risk of overfitting. Overfitting refers to a situation in which the model fits the idiosyncrasies of the training data and loses the ability to generalize from the seen to predict the unseen.\n",
    "[example](https://nbviewer.jupyter.org/urls/s3.amazonaws.com/datarobotblog/notebooks/multiple_regression_in_python.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Variance Trade-Off \n",
    "In order to understand the bias/variance trade off we need to remember what overfitting and underfitting is: \n",
    "\n",
    "> **Underfitting** happens when a model cannot learn the training data, nor can it generalize to new data.\n",
    "\n",
    "> **Overfitting** happens when a model learns the training data too well. In fact, so well that it is not generalizeable to new data\n",
    "\n",
    "Mathematically, if we look at the equation of MSE(mean of our sum of squared errors)\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\n",
    "\n",
    "We can apply bias and variance and it would like this: \n",
    "\n",
    "$ MSE = Bias(\\hat{f}(x))^2 + Var(\\hat{f}(x)) + \\sigma^2$\n",
    "\n",
    "Bias is usually associated with low model complexity, variance with high model complexity. \n",
    "\n",
    "![](https://files.ai-pool.com/a/eba93f5a75070f0fbb9d86bec8a009e9.png)\n",
    "\n",
    "**So, what do you do if you have high bias or variance?** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had high bias and low variance what should we do?\n",
    "- Get rid of outliers \n",
    "- Increase model complexity by adding polynomial features \n",
    "- Get more data OR get more features(feature engineering) \n",
    "\n",
    "If we had low bias and high variance what should we do? \n",
    "- decrease the complexity \n",
    "- reduce the size of your training set\n",
    "- reduce the variables \n",
    "- Get rid of outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Resource for understanding OLS](https://setosa.io/ev/ordinary-least-squares-regression/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
