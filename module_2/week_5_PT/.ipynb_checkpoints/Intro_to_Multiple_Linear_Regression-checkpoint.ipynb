{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-Multiple-Linear-Regression\" data-toc-modified-id=\"Introduction-to-Multiple-Linear-Regression-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction to Multiple Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Goals\" data-toc-modified-id=\"Goals-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Goals</a></span></li><li><span><a href=\"#Review---Simple-Linear-Regression\" data-toc-modified-id=\"Review---Simple-Linear-Regression-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Review - Simple Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Questions-About-the-Advertising-Data\" data-toc-modified-id=\"Questions-About-the-Advertising-Data-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Questions About the Advertising Data</a></span></li><li><span><a href=\"#SciKit-Learn\" data-toc-modified-id=\"SciKit-Learn-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>SciKit Learn</a></span></li></ul></li><li><span><a href=\"#Multiple-Linear-Regression\" data-toc-modified-id=\"Multiple-Linear-Regression-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Multiple Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Statsmodels\" data-toc-modified-id=\"Statsmodels-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Statsmodels</a></span></li><li><span><a href=\"#Scikit-Learn\" data-toc-modified-id=\"Scikit-Learn-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Scikit Learn</a></span></li><li><span><a href=\"#Lets-talk-about-statistics\" data-toc-modified-id=\"Lets-talk-about-statistics-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Lets talk about statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Confidence\" data-toc-modified-id=\"Confidence-1.3.3.1\"><span class=\"toc-item-num\">1.3.3.1&nbsp;&nbsp;</span>Confidence</a></span></li><li><span><a href=\"#p-value\" data-toc-modified-id=\"p-value-1.3.3.2\"><span class=\"toc-item-num\">1.3.3.2&nbsp;&nbsp;</span>p-value</a></span></li><li><span><a href=\"#R-squared\" data-toc-modified-id=\"R-squared-1.3.3.3\"><span class=\"toc-item-num\">1.3.3.3&nbsp;&nbsp;</span>R-squared</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-Validation\" data-toc-modified-id=\"Model-Validation-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Model Validation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train/Test-Split\" data-toc-modified-id=\"Train/Test-Split-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Train/Test Split</a></span></li><li><span><a href=\"#K-Fold-Cross-Validation\" data-toc-modified-id=\"K-Fold-Cross-Validation-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>K-Fold Cross Validation</a></span></li></ul></li><li><span><a href=\"#Categorical-Variables-and-the-Dummy-Trap\" data-toc-modified-id=\"Categorical-Variables-and-the-Dummy-Trap-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Categorical Variables and the Dummy Trap</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-trap\" data-toc-modified-id=\"The-trap-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>The trap</a></span></li></ul></li><li><span><a href=\"#Dealing-with-Multicollinearity\" data-toc-modified-id=\"Dealing-with-Multicollinearity-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Dealing with Multicollinearity</a></span><ul class=\"toc-item\"><li><span><a href=\"#Review\" data-toc-modified-id=\"Review-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Review</a></span></li><li><span><a href=\"#Tomorrow\" data-toc-modified-id=\"Tomorrow-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Tomorrow</a></span></li></ul></li><li><span><a href=\"#Review---Types-of-Variables\" data-toc-modified-id=\"Review---Types-of-Variables-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Review - Types of Variables</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:30:47.852050Z",
     "start_time": "2020-04-15T15:30:47.849753Z"
    }
   },
   "source": [
    "# Introduction to Multiple Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals \n",
    "* Quickly review simple regression with the advertising dataset \n",
    "* Introduce Sklearn and multiple regression  \n",
    "* Review hypothesis testing/confidence intervals/p-values - how do they tie into our regression results \n",
    "* Get an idea of what model validation is with regard to R-squared, underfitting and overfitting \n",
    "* Introduce train-test splits and k-fold cross validation techniques\n",
    "* Learn how to deal with categorical features and multicollinearity \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review - Simple Linear Regression\n",
    "\n",
    "Simple linear regression is an approach for predicting a quantitative output using a single feature (or \"predictor\" or \"input variable\"). It takes the following form:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x$ + _error term_\n",
    "\n",
    "What does each term represent?\n",
    "\n",
    "* $y$ is the output\\\n",
    "* $x$ is the feature or input\\\n",
    "* $\\beta_0$ is the y-intercept\\\n",
    "* $\\beta_1$ is the coefficient for x\\\n",
    "Together, $\\beta_0$ and $\\beta_1$ are called the model coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:58:18.990926Z",
     "start_time": "2020-04-15T13:58:18.949423Z"
    }
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#this allows plots to appear directly in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#read data into a DataFrame\n",
    "data = pd.read_csv('Advertising.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the features/predictors?**\n",
    "\n",
    "*TV:* advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "\n",
    "*Radio:* advertising dollars spent on Radio\n",
    "\n",
    "*Newspaper:* advertising dollars spent on Newspaper\n",
    "\n",
    "**What is the target?**\n",
    "\n",
    "*Sales:* sales of a single product in a given market (in thousands of widgets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions About the Advertising Data\n",
    "\n",
    "Let's pretend you work for the company that manufactures and markets this widget. The company might ask you the following: On the basis of this data, how should we spend our advertising money in the future?\n",
    "\n",
    "This general question might lead you to more specific questions:\n",
    "\n",
    "* Is there a relationship between ads and sales?\n",
    "* How strong is that relationship?\n",
    "* Which ad types contribute to sales?\n",
    "* What is the effect of each ad type of sales?\n",
    "* Given ad spending in a particular market, can sales be predicted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:50:27.464621Z",
     "start_time": "2020-04-15T13:50:27.461328Z"
    }
   },
   "outputs": [],
   "source": [
    "# print the shape of the DataFrame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:50:34.046310Z",
     "start_time": "2020-04-15T13:50:33.694949Z"
    }
   },
   "outputs": [],
   "source": [
    "#There are 200 observations, and thus 200 markets in the dataset.\n",
    "\n",
    "# visualize the relationship between the features and the response using scatterplots\n",
    "fig, axs = plt.subplots(1, 3,)\n",
    "data.plot(kind='scatter', x='TV', y='Sales', ax=axs[0], figsize=(16, 8))\n",
    "data.plot(kind='scatter', x='Radio', y='Sales', ax=axs[1])\n",
    "data.plot(kind='scatter', x='Newspaper', y='Sales', ax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:50:46.095084Z",
     "start_time": "2020-04-15T13:50:45.342460Z"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:50:54.741019Z",
     "start_time": "2020-04-15T13:50:54.707820Z"
    }
   },
   "outputs": [],
   "source": [
    "# formula: target ~ predictor \n",
    "f = 'Sales ~ TV'\n",
    "model = ols(formula=f, data=data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciKit Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression \n",
    "We are still predicting a single variable(y) but now we are using multiple features(xs). This introduces several additional complexities but it also provides a great deal of additional flexibility and predictability.\n",
    "\n",
    "**Examples**: \n",
    "* Your Credit score \n",
    "* What else? \n",
    "\n",
    "**Modified formula**:\n",
    "$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$ \n",
    "\n",
    "So, $n$ is the number of predictors, $\\beta_0$ is the intercept, and $\\hat y$ is the so-called \"fitted line\" or the predicted value associated with the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets talk about statistics \n",
    "\n",
    "#### Confidence\n",
    "A closely related concept is confidence intervals\n",
    "\n",
    "Statsmodels calculates 95% confidence intervals for our model coefficients, which are interpreted as follows:\n",
    "If the population from which this sample was drawn was sampled 100 times approximately 95 of those confidence intervals would contain the \"true\" coefficient\n",
    "\n",
    "We only have a single sample of data, and not the entire population of data. The \"true\" coefficient is either within this interval or it isn't, but there's no way to actually know. We estimate the coefficient with the data we do have, and we show uncertainty about that estimate by giving a range that the coefficient is probably within.\n",
    "_Note that using 95% confidence intervals is just a convention_\n",
    "\n",
    "You can create 90% confidence intervals (which will be more narrow)\n",
    "99% confidence intervals (which will be wider) or whatever intervals you like.\n",
    "\n",
    "\n",
    "#### p-value\n",
    "\n",
    "Represents the probability that the coefficient is actually zero\n",
    "\n",
    "**Interpreting p-values**\n",
    "\n",
    "If the 95% confidence interval does not include _zero_ p-value will be less than 0.05 and you should reject the null \n",
    "**Question: Why not zero?**\n",
    "\n",
    "If the 95% confidence interval includes zero p-value for that coefficient will be greater than 0.05.\n",
    "Fail to reject the null.\n",
    "There is no relationship\n",
    "\n",
    "**Notes**\n",
    "\n",
    "A p-value less than 0.05 is one way to decide whether there is likely a relationship between the feature and the response.\n",
    "In this case, the p-value for TV is far less than 0.05.\n",
    "There is a low probability that the coefficient is actually zero.\n",
    "Reject null hypothesis.\n",
    "There is a relationship.\n",
    "\n",
    "\n",
    "TV and Radio have small p-values, whereas Newspaper has a large p-value.\n",
    "Reject the null hypothesis for TV and Radio.\n",
    "There is association between features and Sales.\n",
    "Fail to reject the null hypothesis for Newspaper.\n",
    "There is no association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-squared\n",
    "To evaluate the overall fit of a linear model, we use the R-squared value\n",
    "\n",
    "R-squared is the proportion of variance explained\n",
    "It is the proportion of variance in the observed data that is explained by the model, or the reduction in error over the null model\n",
    "The null model just predicts the mean of the observed response, and thus it has an intercept and no slope\n",
    "R-squared is between 0 and 1\n",
    "Higher values are better because it means that more variance is explained by the model.\n",
    "\n",
    "The actual calculation of $R^2$ is:\n",
    "$\\Large R^2\\equiv 1-\\frac{\\Sigma_i(y_i - \\hat{y}_i)^2}{\\Sigma_i(y_i - \\bar{y})^2}$.\n",
    "\n",
    "$R^2$ is a measure of how much variation is in the dependent variable your model explains.\n",
    "\n",
    "Adjusted $R^2$\n",
    "There are some theoretical objections to using $R^2$ as an evaluator of a regression model.\n",
    "\n",
    "One objection is that, if we add another predictor to our model, $R^2$ can only increase! (It could hardly be that with more features I'd be able to account for less of the variation in the dependent variable than I could with the smaller set of features. We saw this with adding Newspaper ads.)\n",
    "\n",
    "One improvement is adjusted $R^2$:\n",
    "$\\Large R^2_{adj.}\\equiv 1 - \\frac{(1 - R^2)(n - 1)}{n - m - 1}$, where:\n",
    "\n",
    "n is the number of data points; and\n",
    "m is the number of predictors.\n",
    "This can be a better indicator of the quality of a regression model.\n",
    "\n",
    "R-squared will always increase as you add more features to the model, even if they are unrelated to the response\n",
    "Selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "* Adjusted R-squared\n",
    "Penalizes model complexity (to control for overfitting), but it generally under-penalizes complexity.\n",
    "\n",
    "**Better Solution**\n",
    "\n",
    "* Train/test split or cross-validation\n",
    "More reliable estimate.\n",
    "Better for choosing which of your models will best generalize to other data.\n",
    "There is extensive functionality for cross-validation in scikit-learn, including automated methods for searching different sets of parameters and different models.\n",
    "Cross-validation can be applied to any model, whereas the methods described above only apply to linear models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation \n",
    "Our model is useless unless it generalizes well when we add new data. But how can we tell if our model is performing well? \n",
    "\n",
    "Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.\n",
    "\n",
    "![](https://docs.aws.amazon.com/machine-learning/latest/dg/images/mlconcepts_image5.png)\n",
    "\n",
    "**To evaluate our training and test sets, we will compare our $\\hat y$ with the actual value, $y$.**\n",
    " \n",
    "**Mean Squared Error (MSE)** is the mean of the squared errors:\n",
    "\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\n",
    " \n",
    "**Root Mean Squared Error (RMSE)** is the square root of the mean of the squared errors:\n",
    "\n",
    "$\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split \n",
    "We need to split our data into a training and a testing set, randomly. A 70/30 split is pretty standard but what you choose can depend on the model you are using.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:02:14.929960Z",
     "start_time": "2020-04-15T15:02:14.927337Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate and fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation \n",
    "The general procedure is as follows:\n",
    "\n",
    "1. Shuffle the dataset randomly.\n",
    "2. Split the dataset into k groups\n",
    "3. For each unique group:\n",
    "    * Take the group as a hold out or test data set\n",
    "    * Take the remaining groups as a training data set\n",
    "    * Fit a model on the training set and evaluate it on the test set\n",
    "    * Retain the evaluation score and discard the model\n",
    "4. Summarize the skill of the model using the sample of model evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables and the Dummy Trap\n",
    "\n",
    "Up to now, all of our features have been numeric. What if one of our features was categorical?\n",
    "\n",
    "What is a categorical feature?\n",
    "**Examples**\n",
    "* gender\n",
    "* region/country\n",
    "\n",
    "Let's create a new feature called Size(market size), and randomly assign observations to be small or large:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:40:27.112450Z",
     "start_time": "2020-04-15T15:40:27.098417Z"
    }
   },
   "outputs": [],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(12345)\n",
    "\n",
    "# create a Series of booleans in which roughly half are True\n",
    "nums = np.random.rand(len(data))\n",
    "mask_large = nums > 0.5\n",
    "\n",
    "# initially set Size to small, then change roughly half to be large\n",
    "data['Size'] = 'small'\n",
    "\n",
    "# Series.loc is a purely label-location based indexer for selection by label\n",
    "data.loc[mask_large, 'Size'] = 'large'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scikit-learn, we need to represent all data numerically\n",
    "\n",
    "If the feature only has two categories, we can simply create a dummy variable that represents the categories as a binary value, 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:40:42.837624Z",
     "start_time": "2020-04-15T15:40:42.826686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Manually - create a new Series called Size_large\n",
    "data['Size_large'] = data.Size.map({'small':0, 'large':1})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:41:46.777664Z",
     "start_time": "2020-04-15T15:41:46.769113Z"
    }
   },
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper', 'Size_large']\n",
    "X = data[feature_cols]\n",
    "y = data.Sales\n",
    "\n",
    "# instantiate\n",
    "mlr2 = LinearRegression()\n",
    "# fit\n",
    "mlr2.fit(X, y)\n",
    "\n",
    "# print coefficients\n",
    "list(zip(feature_cols, mlr2.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the Size_large coefficient**\n",
    "\n",
    "* For a given amount of TV/Radio/Newspaper ad spending, being a large market is associated with an average increase in Sales of 57.42 widgets (as compared to a small market, which is called the baseline level).\n",
    "* What if we had reversed the 0/1 coding and created the feature 'Size_small' instead?\n",
    "* The coefficient would be the same, except it would be negative instead of positive\n",
    "* As such, your choice of category for the baseline does not matter, all that changes is your interpretation of the coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new feature called Area, and randomly assign observations to be rural, suburban, or urban:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:43:09.803783Z",
     "start_time": "2020-04-15T15:43:09.787933Z"
    }
   },
   "outputs": [],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(123456)\n",
    "\n",
    "# assign roughly one third of observations to each group\n",
    "nums = np.random.rand(len(data))\n",
    "mask_suburban = (nums > 0.33) & (nums < 0.66)\n",
    "mask_urban = nums > 0.66\n",
    "data['Area'] = 'rural'\n",
    "# Series.loc is a purely label-location based indexer for selection by label\n",
    "data.loc[mask_suburban, 'Area'] = 'suburban'\n",
    "data.loc[mask_urban, 'Area'] = 'urban'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:43:16.075048Z",
     "start_time": "2020-04-15T15:43:16.063910Z"
    }
   },
   "outputs": [],
   "source": [
    "# create three dummy variables using get_dummies\n",
    "pd.get_dummies(data.Area, prefix='Area').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The trap\n",
    "\n",
    "Due to the nature of how dummy variables are created, one variable can be predicted from all of the others. This is known as perfect multicollinearity and it can be a problem for regression. Multicollinearity will be covered in depth later but the basic idea behind perfect multicollinearity is that you can perfectly predict what one variable will be using some combination of the other variables.\n",
    "\n",
    "However, we actually only need two dummy variables, not three. Why? Because two dummies captures all of the \"information\" about the Area feature, and implicitly defines rural as the \"baseline level\".\n",
    "\n",
    "Let's see what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:44:22.193610Z",
     "start_time": "2020-04-15T15:44:22.185477Z"
    }
   },
   "outputs": [],
   "source": [
    "# create three dummy variables using get_dummies, then exclude the first dummy column\n",
    "area_dummies = pd.get_dummies(data.Area, prefix='Area').iloc[:, 1:] #drop first=True\n",
    "area_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we interpret the coding:\n",
    "\n",
    "* rural is coded as Area_suburban=0 and Area_urban=0\n",
    "* suburban is coded as Area_suburban=1 and Area_urban=0\n",
    "* urban is coded as Area_suburban=0 and Area_urban=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Multicollinearity\n",
    "The interpretation of a regression coefficient is that it represents the average change in the dependent variable for each 1 unit change in a predictor, assuming that all the other predictor variables are kept constant. Multicollinearity occurs when 2 or more of the independent variables are higly correlated with each other.\n",
    "\n",
    "**How do we tell if variables are correlated with each other?**\n",
    "1. Look at a scatter matrix \n",
    "2. Look at a heatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:47:20.148685Z",
     "start_time": "2020-04-15T15:47:19.996056Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(data.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:47:31.612445Z",
     "start_time": "2020-04-15T15:47:30.650263Z"
    }
   },
   "outputs": [],
   "source": [
    "'''This matrix has the cool feature that it returns scatterplots\n",
    "for relationships between two predictors, and histograms for a \n",
    "single feature on the diagonal.'''\n",
    "pd.plotting.scatter_matrix(data);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "![](https://convertwithcontent.com/wp-content/uploads/2014/04/review-the-results-of-your-a-b-split-test-300x225.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tomorrow\n",
    "We will continue to build our understanding of regression by taking a look at methods to evaluate and improve upon our models. We will discuss scaling, transformations, interactions and the bias/variance tradeoff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review - Types of Variables \n",
    "[ALL THE VARIABLES](https://www.statisticshowto.com/probability-and-statistics/types-of-variables/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
