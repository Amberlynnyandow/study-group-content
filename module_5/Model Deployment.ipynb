{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Traditional methods \n",
    "![](https://dz2cdn1.dzone.com/storage/temp/14016093-api-collaboration.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pickling for Deployment Via an API\n",
    "\n",
    "This notebook shows the basic outline for training a model, evaluating it, then using it in a \"production\" context to make predictions about new data.\n",
    "\n",
    "![](https://miro.medium.com/max/373/1*Menz4NWvM6Ca8H6B0d6VrQ.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Serializing is a way to write a python object on the disk that can be transferred anywhere and later de-serialized (read) back by a python script.\n",
    "\n",
    "The majority of Machine learning engineers use R / Python for their experiments. But consumers of those ML models would be software engineers who use a completely different stack. There are two ways via which this problem can be solved:\n",
    "\n",
    "**Option 1:** Rewriting the whole code in the language that the software engineering folks work. The above seems like a good idea, but the time & energy required to get those intricate models replicated would be utterly waste. Majority of languages like JavaScript, do not have great libraries to perform ML. One would be wise to stay away from it.\n",
    "\n",
    "**Option 2** –Use a Web API. APIs have made it easy for cross-language applications to work well. If a frontend developer needs to use your ML Model to create a ML powered web application, they would just need to get the URL Endpoint from where the API is being served."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project Structure\n",
    "\n",
    "1. **model.py** — This contains code for the machine learning model to predict sales in the third month based on the sales in the first two months.\n",
    "<br/>\n",
    "2. **app.py** — This contains Flask APIs that receives sales details through GUI or API calls, computes the predicted value based on our model and returns it.\n",
    "<br/>\n",
    "3. **request.py** — This uses requests module to call APIs defined in app.py and displays the returned value.\n",
    "<br/>\n",
    "4. **HTML/CSS** — This contains the HTML template and CSS styling to allow user to enter sales detail and displays the predicted sales in the third month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### app.py file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The next part is to make an API which receives inputs through the GUI and computes the predicted values based on our model. For this, you de-serialize the pickled model in the form of a python object. I set the main page using index.html. On submitting the form values use POST request to /predict, we get the predicted value.\n",
    "\n",
    "The results can be shown by making another POST request to /results. It receives JSON inputs, uses the trained model to make a prediction and returns that prediction in JSON format which can be accessed through the API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict',methods=['POST'])\n",
    "def predict():\n",
    "\n",
    "    int_features = [int(x) for x in request.form.values()]\n",
    "    final_features = [np.array(int_features)]\n",
    "    prediction = model.predict(final_features)\n",
    "\n",
    "    output = round(prediction[0], 2)\n",
    "\n",
    "    return render_template('index.html', prediction_text='Sales should be $ {}'.format(output))\n",
    "\n",
    "@app.route('/results',methods=['POST'])\n",
    "def results():\n",
    "\n",
    "    data = request.get_json(force=True)\n",
    "    prediction = model.predict([np.array(list(data.values()))])\n",
    "\n",
    "    output = prediction[0]\n",
    "    return jsonify(output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HTML/CSS \n",
    "What the user sees when they go to the applications web-site. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<!DOCTYPE html>\n",
    "<html >\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <title>Deployment Tutorial 1</title>\n",
    "  <link href='https://fonts.googleapis.com/css?family=Pacifico' rel='stylesheet' type='text/css'>\n",
    "<link href='https://fonts.googleapis.com/css?family=Arimo' rel='stylesheet' type='text/css'>\n",
    "<link href='https://fonts.googleapis.com/css?family=Hind:300' rel='stylesheet' type='text/css'>\n",
    "<link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>\n",
    "<link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/style.css') }}\">\n",
    "  \n",
    "</head>\n",
    "\n",
    "<body style=\"background: #000;\">\n",
    " <div class=\"login\">\n",
    "\t<h1>Sales Forecasting</h1>\n",
    "\n",
    "     <!-- Main Input For Receiving Query to our ML -->\n",
    "    <form action=\"{{ url_for('predict')}}\"method=\"post\">\n",
    "    \t<input type=\"text\" name=\"rate\" placeholder=\"rate\" required=\"required\" />\n",
    "        <input type=\"text\" name=\"sales in first month\" placeholder=\"sales in first month\" required=\"required\" />\n",
    "\t\t<input type=\"text\" name=\"sales in second month\" placeholder=\"sales in second month\" required=\"required\" />\n",
    "        <button type=\"submit\" class=\"btn btn-primary btn-block btn-large\">Predict sales in third month</button>\n",
    "    </form>\n",
    "\n",
    "   <br>\n",
    "   <br>\n",
    "   {{ prediction_text }}\n",
    "\n",
    " </div>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extract, Transform, Load Data\n",
    "\n",
    "This is easy here because I'm using a nice tidy dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:01:03.152822Z",
     "start_time": "2021-02-11T17:00:57.818509Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:02:30.832097Z",
     "start_time": "2021-02-11T17:02:30.825446Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# get premade wine dataset from sklearn\n",
    "data = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:02:42.514572Z",
     "start_time": "2021-02-11T17:02:42.511009Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(data.DESCR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build a Model to Make Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:03:21.037513Z",
     "start_time": "2021-02-11T17:03:20.970674Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's build a model to predict the class of wine\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=100)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluate the Model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The pickle format is popular for this task in Python. Pickling is a form of serialization or flattening, which basically means converting everything about an object in memory into bits of data that can be stored in a file.\n",
    "\n",
    "**First**, we create a pickle object for our classifier.<br/> \n",
    "**Second**, we use the pickle.dump method to convert a Python object hierarchy into a byte stream. This process is also called as serilaization.\n",
    "\n",
    "*pickle.dump(pythonObject, pickleDestination, pickle_protocol=None, *, fix_imports=True)* \n",
    "\n",
    "**Third**, close the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:05:00.706281Z",
     "start_time": "2021-02-11T17:05:00.700582Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "output_file = open(\"wine_classifier.pickle\", \"wb\") # \"wb\" means \"write as bytes\"\n",
    "pickle.dump(classifier, output_file)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load the Model \n",
    "\n",
    "The goal is to take information that was stored in memory at one time, then save it so it can be used later. Here specifically this is useful because training a model is usually a lot slower than using the model to make a prediction, so this saves us from having to re-run that costly operation each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:17:33.751585Z",
     "start_time": "2021-02-11T17:17:33.743508Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_file = open(\"wine_classifier.pickle\", \"rb\") # \"rb\" means \"read as bytes\"\n",
    "loaded_model = pickle.load(model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Make a Prediction with the Loaded Model \n",
    "\n",
    "In this section I'm constructing a request JSON that resembles what would come from a user who wants a predicted class of wine based on these feature values. This code would not actually exist in your deployed application, it would be created automatically by whatever protocol generated the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:18:48.559078Z",
     "start_time": "2021-02-11T17:18:48.552204Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# make a fake request JSON from the user with all the headings\n",
    "request_json = {}\n",
    "\n",
    "expected_features = (\"Alcohol\", \"Malic acid\", \"Ash\", \"Alcalinity of ash\", \\\n",
    "        \"Magnesium\", \"Total phenols\", \"Flavanoids\", \"Nonflavanoid phenols\", \\\n",
    "        \"Proanthocyanins\", \"Color intensity\", \"Hue\", \\\n",
    "        \"OD280/OD315 of diluted wines\", \"Proline\")\n",
    "example_values = [1.282e+01, 3.370e+00, 2.300e+00, 1.950e+01, 8.800e+01, 1.480e+00, \\\n",
    "       6.600e-01, 4.000e-01, 9.700e-01, 1.026e+01, 7.200e-01, 1.750e+00, \\\n",
    "       6.850e+02]\n",
    "\n",
    "for i, feature in enumerate(expected_features):\n",
    "    request_json[feature] = example_values[i]\n",
    "request_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is the section that more closely resembles what you might have in your application. I'm checking to make sure that the expected values are in the request_json, transforming them into the right format to make a prediction, then printing out that prediction. In your actual deployed code, you would most likely be returning the response, not printing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:19:50.258607Z",
     "start_time": "2021-02-11T17:19:50.248178Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if request_json and all(feature in request_json for feature in expected_features):\n",
    "    # unpack all of the relevant values from the request into a list\n",
    "    test_value = [request_json[feature] for feature in expected_features]\n",
    "    \n",
    "    # make a prediction from the \"user input\"\n",
    "    predicted_class = int(loaded_model.predict([test_value])[0])\n",
    "    \n",
    "    # construct a response\n",
    "    response_json = {\"prediction\": predicted_class}\n",
    "    print(response_json)\n",
    "else:\n",
    "    print(\"something was missing from the request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Productionizing Models as a Career Skill\n",
    "1. Many data scientists don't know how to put machine learning models into production.  \n",
    "2. Putting a model into production is a mandatory skill for data scientists at most small to medium-sized companies.\n",
    "3. Being able to productionize models will make you a much more attractive candidate to employers, and give you a competitive advantage!\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-data-science-and-machine-learning-engineering-online-ds-ft-100719/master/images/new-venn-diagram.png\" width=60%>\n",
    "\n",
    "> -  A decade ago, productionizing a machine learning model would have meant building your own web server with something like [Flask](http://flask.pocoo.org/) or [Django](https://www.djangoproject.com/) and hosting somewhere, just like you would with any web app. \n",
    "> - Now, we don't even need to worry about things like server code -- instead, we can use preexisting services from AWS that were created specifically to simplify the process of productionizing machine learning solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cloud Computing\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_pt_100719_cohort_notes/master/Instructor%20Notebooks/sect_43/image/cloud_PNG27.png\" alt=\"image3\" style = \"width:400px\">\n",
    "\n",
    "\"Simply put, cloud computing is the delivery of computing services—including servers, storage, databases, networking, software, analytics, and intelligence—over the Internet (“the cloud”) to offer faster innovation, flexible resources, and economies of scale. You typically pay only for cloud services you use, helping you lower your operating costs, run your infrastructure more efficiently, and scale as your business needs change.\" - Microsoft Azure\n",
    "\n",
    "![microsoft example](https://miro.medium.com/max/624/1*QmV2VDvgIquNx-Daxcdddw.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Most cloud computing services fall into four broad categories: infrastructure as a service (IaaS), platform as a service (PaaS), serverless, and software as a service (SaaS). These are sometimes called the cloud computing \"stack\" because they build on top of one another. Knowing what they are and how they’re different makes it easier to accomplish your business goals.\n",
    "\n",
    "### Solves many problems:\n",
    "\n",
    "- How can I keep my data secure yet accessible remotely?\n",
    "- How can I pay less for software licenses?\n",
    "- What if I need more server space in the future?\n",
    "- I have more data to analyze than can fit on my computer. What can I do?\n",
    "- My model has taken three days to run. Is there a faster way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AWS is the Most Popular & It is Intimidating\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_pt_100719_cohort_notes/master/Instructor%20Notebooks/sect_43//image/aws_focus.png\" alt=\"foci\" style = \"width:90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you want:\n",
    "- to work from a Jupyter notebook locally\n",
    "- to keep your analysis in a Jupyter notebook\n",
    "- to store your work on git as well\n",
    "- to not concerned about access or keeping data private\n",
    "- the easiest and fastest solution to getting our notebook in the cloud\n",
    "\n",
    "### Focus on the last 2 questions \n",
    "- **I have more data to analyze than can fit on my computer. What can I do?**\n",
    "- **My model has taken three days to run. Is there a faster way?**\n",
    "\n",
    "So you will likely only use **S3**, **Sagemaker**, and **IAM**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Storage \n",
    "\n",
    "<img src=\"https://pngimg.com/uploads/bucket/bucket_PNG7777.png\" alt=\"bucket\" style = \"text-align:left;width:200px;float:none\">\n",
    "\n",
    "#### Buckets defined\n",
    "\n",
    "[by PC Mag](https://www.pcmag.com/encyclopedia/term/bucket)\n",
    "\n",
    "> A customer-defined storage area in a cloud-based storage system such as Amazon's S3 or Google Storage. Each bucket can be divided into folders. Customers are not charged for the buckets themselves, only when data reside within them. See S3 cloud storage and Google Storage.\n",
    "\n",
    "#### S3 stands for _Amazon Simple Storage Service_\n",
    "Amazon uses [S3 buckets](https://aws.amazon.com/s3/) for the most general form of object storage.\n",
    "\n",
    "<!---\n",
    "<img src=\"https://cdn.worldvectorlogo.com/logos/aws-s3.svg\"></br>--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Credentials \n",
    "\n",
    "![credentials](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRvaB5OvGWguYHBlVyagwofOP9kX0h5HqtbcIa02MyAVs_XS90McA&s)\n",
    "\n",
    "#### Credentials Defined:\n",
    "\n",
    "[From AWS](https://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html)\n",
    "\n",
    "> When you interact with AWS, you specify your AWS security credentials to verify who you are and whether you have permission to access the resources that you are requesting. AWS uses the security credentials to authenticate and authorize your requests.\n",
    "\n",
    ">For example, if you want to download a specific file from an Amazon Simple Storage Service (Amazon S3) bucket, your credentials must allow that access. If your credentials aren't authorized to download the file, AWS denies your request.\n",
    "\n",
    "#### Our approach to credentials:\n",
    "\n",
    "Make everything public. </br>\n",
    "But we will still have to work with **IAM** a bit to make things talk to each other. \n",
    "\n",
    "<img src=\"https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png\" alt=\"aws\" style =\"text-align:center;width:250px;float:none\" ></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Region\n",
    "\n",
    "<img src=\"https://www.concurrencylabs.com/img/posts/9-choose-region-wisely/choose-your-aws-region.png\" wiodth=30%></br>\n",
    "\n",
    "#### Regions Defined:\n",
    "[from AWS documentation](https://aws.amazon.com/about-aws/global-infrastructure/regions_az/):\n",
    ">AWS has the concept of a Region, which is a physical location around the world where we cluster data centers. We call each group of logical data centers an Availability Zone. Each AWS Region consists of multiple, isolated, and physically separate AZ's within a geographic area...\n",
    "\n",
    ">Each AZ has independent power, cooling, and physical security and is connected via redundant, ultra-low-latency networks. AWS customers focused on high availability can design their applications to run in multiple AZ's to achieve even greater fault-tolerance. AWS infrastructure Regions meet the highest levels of security, compliance, and data protection.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_pt_100719_cohort_notes/master/Instructor%20Notebooks/sect_43//image/aws_regions_facts.png\" alt=\"aws_regions\" style =\"text-align:center;width:500px;float:none\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Each time you create a new \"service\" in AWS, you need to define its region.\n",
    "- Each region is a separate geographic area and is completely independent\n",
    "- Each Amazon region is designed to be completely isolated from the other regions & helps achieve the greatest possible fault tolerance and stability\n",
    "- Communication between regions is across the public Internet and appropriate measures should be taken to protect the data using encryption\n",
    "- Data transfer between regions is charged at the Internet data transfer rate for both the sending and the receiving instance\n",
    "- Resources aren’t replicated across regions unless done explicitly\n",
    "\n",
    "Here are some real factors impacted by your choice of region:\n",
    "\n",
    "    - Latency\n",
    "    - Cost\n",
    "    - Legal Compliance\n",
    "    - Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Amazon Sagemaker\n",
    "<img src=\"https://d2908q01vomqb2.cloudfront.net/77de68daecd823babbb58edb1c8e14d7106e83bb/2018/04/24/SageMaker.jpg\" alt=\"sagemaker\" style =\"text-align:center;width:250px;float:none\" ></br>\n",
    "\n",
    "> ***SageMaker is a platform created by Amazon to centralize all the various services related to Data Science and Machine Learning. If you're a data scientist working on AWS, chances are that you'll be spending most (if not all) of your time in SageMaker getting things done.***\n",
    "\n",
    "\n",
    "> * Amazon has centralized all of the major data science services inside **_Amazon SageMaker_**. SageMaker provides numerous services for things such as:\n",
    "    * Data Labeling\n",
    "    * Cloud-based Notebooks\n",
    "    * Training and Model Tuning\n",
    "    * Inference\n",
    "    \n",
    "#### SageMaker Components\n",
    "All accessible via a web interface through the AWS console. So you need a web connection/browser to get started. The SageMaker command line interface also allows you to manage your project externally. \n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-introduction-to-aws-sagemaker-online-ds-ft-100719/master/images/use_cases.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview of the Process \n",
    "\n",
    "When productionizing a machine learning model using AWS, you'll typically use the following workflow:\n",
    "\n",
    "1. Explore and preprocess data\n",
    "2. Build SageMaker container (Docker)\n",
    "3. Test training and inference code on your local machine \n",
    "4. Train and deploy model with SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Resources \n",
    "   - [Canvas Lesson: AWS Ecosystem](https://github.com/learn-co-curriculum/dsc-the-aws-ecosystem) (has account/IAM setup)\n",
    "   - [Canvas Lesson: Productionizing Models with SageMaker](https://learn.co/tracks/module-4-data-science-career-2-1/big-data-deep-learning-and-natural-language-processing/section-43-operationalizing-code-and-aws/productionizing-a-model-with-docker-and-sagemaker)\n",
    "   - [Labs to complete](https://github.com/aws-samples/amazon-sagemaker-keras-text-classification)\n",
    "   - [30min Video walk-thru of Produtionizing Models with SageMaker](https://www.youtube.com/watch?v=wZ2G9erPX00)\n",
    "   - **[AWS: Official SageMaker TutorialRepo](https://github.com/aws-samples/amazon-sagemaker-keras-text-classification)**\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
